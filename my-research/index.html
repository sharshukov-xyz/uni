<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>НИР</title>

    <link href="/assets/css/styles.css" rel="stylesheet">
    <script defer src="/assets/js/core.js"></script>
    <script defer src="/assets/js/questions.js"></script>
    <script defer src="/assets/js/controls.js"></script>

    <link rel="stylesheet" href="/assets/katex/katex.min.css">
    <script defer src="/assets/katex/katex.min.js"></script>
    <script defer src="/assets/katex/auto-render.min.js"></script>

    <script>
      document.addEventListener("DOMContentLoaded", function () {
          renderMathInElement(document.body, {
              // customised options
              // • auto-render specific keys, e.g.:
              delimiters: [
                  { left: '$$', right: '$$', display: true },
                  { left: '$', right: '$', display: false },
                  { left: '\\(', right: '\\)', display: false },
                  { left: '\\[', right: '\\]', display: true }
              ],
              // • rendering keys, e.g.:
              throwOnError: false
          });
      });
    </script>
  </head>

  <body>
    <div style="display:none">
      $\global\def\at#1#2{\left. #1 \right\rvert_{#2}}$
      $\global\def\abs#1{\left\lvert #1 \right\rvert}$
      $\global\def\norm#1{\left\lVert #1 \right\rVert}$
      $\global\def\bvec#1{\mathbf{#1}}$
      $\global\def\floor#1{\left\lfloor #1 \right\rfloor}$

      $\global\def\limto#1{\underset{#1}{\longrightarrow}}$
      $\global\def\prob#1{\mathbb{P} \left\{ #1 \right\}}$
      $\global\def\mean#1{\mathbb{E} \left[ #1 \right]}$
      $\global\def\disp#1{D \left[ #1 \right]}$

      $\global\def\dp#1#2{#1 \cdot #2\,}$
      $\global\def\vp#1#2{#1 \times #2\,}$

      $\global\def\dv#1#2{\frac{d #1}{d #2}}$
      $\global\def\ddv2#1#2{\frac{d^2 #1}{d #2^2}}$
      $\global\def\rdv#1#2{\frac{d' #1}{d #2}}$ <!-- относительная производная -->
      $\global\def\pd#1#2{\frac{\partial #1}{\partial #2}}$
      $\global\def\pdv2#1#2{\frac{\partial^2 #1}{\partial #2^2}}$
      $\global\def\pdvk#1#2#3{\frac{\partial^#1 #2}{\partial #3^#1}}$
      $\global\def\ppdv#1#2#3{\frac{\partial^2 #1}{\partial #2 \partial #3}}$

      $\global\def\pois#1{\left\{ #1 \right\}}$
      $\global\def\paren#1{\left( #1 \right)}$
      $\global\def\bydef#1{\overset{\mathrm{def}}{#1}}$

      $\global\def\mbox#1{\text{#1}}$

      $\global\def\div{\text{div}\,}$
      $\global\def\dsum{\displaystyle\sum\,}$
      $\global\def\grad{\text{grad}\,}$
      $\global\def\rot{\text{rot}\,}$

      $\global\def\vb#1{\textbf{#1}}$

      $\global\def\op#1{\mathrm{#1}\,}$

      $\global\def\Im{\text{Im}\,}$
      $\global\def\Res{\text{Res}\,}$
      $\global\def\Re{\text{Re}\,}$
      $\global\def\argtg{\text{argtg}\,}$
      $\global\def\ch{\text{ch}\,}$
      $\global\def\const{\text{const}\,}$
      $\global\def\degree{\text{degree}\,}$
      $\global\def\proj{\mathrm{proj}}$
      $\global\def\rank{\mathrm{rank}}$
      $\global\def\res{\text{res}\,}$
      $\global\def\sh{\text{sh}\,}$
      $\global\def\sign{\text{sign}\,}$
      $\global\def\tg{\mathrm{tg}\,}$

      $\global\def\C{\mathbb{C}}$
    </div>

    <h1 id="title">Научно-исследовательская работа</h1>
    <nav>
      <a href="/">Домой</a>
      <a href="/links">Ссылки</a>
      <a href="/about">Контакты</a>
    </nav>

    <h1>Time-Delay Systems</h1>

    <div class="content">
      <h2>Общая теория</h2>
      <div class="todo">
        написать.
      </div>
    </div>

    <br />

    <div class="content">
      <h2>Случай одного запаздывания</h2>
      <h3>Введение</h3>
      Будем рассматривать систему
      \[
      \tag{2.1}
      \dv{x(t)}{t} = A_0 x(t) + A_1 x(t - h), \qquad t \geqslant 0,
      \]
      где $A_0, A_1 \in \R^{n \times n}$ &mdash; заданные вещественные матрицы,
      а $h \in \R, h \gt 0$ &mdash; запаздывание.

      <p>
        Пусть $\varphi: [-h, 0] \to \R^n$ &mdash; начальная функция. Будем предполагать,
        что она является кусочно-непрерывной, то есть $\varphi \in PC([-h, 0], \R^n)$.
      </p>

      <p>
        Пусть $x(t, \varphi)$ &mdash; решение системы $(2.1)$, удовлетворяющее начальной
        функции $\varphi$:
        \[
        x(\theta, \varphi) = \varphi(\theta), \qquad \theta \in [-h, 0].
        \]
        Также через $x_t(\varphi)$ будем обозначать сужение этого решения на отрезок
        $[t - h, t]$:
        \[
        x_t(\varphi): \theta \to x(t + \theta, \varphi), \qquad \theta \in [-h, 0].
        \]
      </p>

      <p>
        Будем использовать евклидову норму для векторов и индуцированную норму для
        матриц. Для элементов пространства $PC([-h, 0], \R^n)$ будем использовать
        равномерную норму:
        \[
        \norm{\varphi}_h = \sup\limits_{\theta \in [-h, 0]} \norm{\varphi(\theta)}.
        \]
        Поставим задачу отсыскания явного выражения для решения системы $(2.1)$
        через начальные условия.
      </p>

      <h4>Фундаментальная матрица</h4>
      <div class="definition">
        Матрица $K(t) \in \R^{n \times n}$ называется <i>фундаментальной матрицей</i>
        системы $(2.1)$, если
        \[
        \tag{2.2}
        \dv{}{t} K(t) = K(t) A_0 + K(t - h) A_1, \qquad t \geqslant 0,
        \]
        причём $K(t) = 0_{n \times n}$ при $t \lt 0$ и $K(0) = I$.
      </div>

      <div class="remark">
        Фундаментальная матрица также удовлетворяет матричному уравнению
        \[
        \dv{}{t} K(t) = A_0 K(t) + A_1 K(t - h), \qquad t \geqslant 0.
        \]
        Отсюда, вообще говоря, не следует, что матрица $K(t)$ коммутирует с матрицами
        $A_0,A_1$ по отдельности.
      </div>

      <h4>Формула Коши</h4>
      <div class="theorem">
        Для начальной функции $\varphi \in PC([-h, 0], \R^n)$ справедливо равенство
        \[
        \tag{2.3}
        x(t, \varphi) = K(t) \varphi(0)
        +
        \int\limits_{-h}^{0} K(t - \theta - h) A_1 \varphi(\theta) d\theta,
        \qquad t \geqslant 0.
        \]
        Это выражение для $x(t, \varphi)$ называют <i>формулой Коши</i>.

        <div class="proof">
          Пусть $t \gt 0$. Возьмём некоторое число $\xi \in (0, t)$; тогда
          \[
          \begin{aligned}
          \pd{}{\xi} \paren{ K(t - \xi) x(\xi, \varphi) }
          &=
          - \left[
          K(t - \xi) A_0 + K(t - \xi - h) A_1
          \right] x(\xi, \varphi) + \\
          &\phantom{=}
          + K(t - \xi) \left[
          A_0 x(\xi, \varphi) + A_1 x(\xi - h, \varphi)
          \right].
          \end{aligned}
          \]
          Интегрируя это выражение по $\xi$ от $0$ до $t$, получаем
          \[
          x(t, \varphi) - K(t) \varphi(0)
          =
          - \int\limits_{0}^{t} K(t - \xi - h) A_1 x(\xi, \varphi) d\xi
          + \int\limits_{0}^{t} K(t - \xi) A_1 x(\xi - h, \varphi) d\xi.
          \]
          Второй интеграл в правой части можно представить как
          \[
          \int\limits_{0}^{t} K(t - \xi) A_1 x(\xi - h, \varphi) d\xi
          =
          \left[ \theta := \xi - h \right]
          =
          \int\limits_{-h}^{t - h} K(t - \theta - h) A_1 x(\theta, \varphi) d\theta.
          \]
          Так как матрица $K(\theta) = 0_{n \times n}$ при $\theta \in [-h, 0)$,
          верхний предел интегрирования может быть увеличен до $t$:
          \[
          \int\limits_{-h}^{t - h} K(t - \theta - h) A_1 x(\theta, \varphi) d\theta
          =
          \int\limits_{-h}^{t} K(t - \theta - h) A_1 x(\theta, \varphi) d\theta.
          \]
          Итак, окончательно получаем, что
          \[
          x(t, \varphi)
          =
          K(t) \varphi(0)
          +
          \int\limits_{-h}^{0} K(t - \theta - h) A_1 x(\theta, \varphi) d\theta,
          \qquad t \geqslant 0.
          \]
          Так как $x(\theta, \varphi) = \varphi(\theta)$ при $\theta \in [-h, 0]$,
          то окончательно получаем формулу Коши.
        </div>
      </div>

      <h3>Экспоненциальная устойчивость</h3>

      <div class="definition">
        Говорят, что система $(2.1)$ <i>экспоненциально устойчива</i>, если найдутся
        $\gamma \geqslant 1, \sigma \gt 0$ такие, что любое решение $x(t, \varphi)$
        удовлетворяет неравенству
        \[
        \tag{2.4}
        \norm{x(t, \varphi)} \leqslant \gamma e^{-\sigma t} \norm{\varphi}_h,
        \qquad t \geqslant 0.
        \]
      </div>

      <div class="remark">
        Известно, что из экспоненциальной устойчивости системы $(2.1)$ следует её
        асимптотическая устойчивость.
      </div>

      <div class="definition">
        Комплексное число $s_0 \in \C$ называют <i>собственным числом</i> системы $(2.1)$,
        если оно является корнем характеристической функции системы
        \[
        f(s) = \det\paren{ sI - A_0 - e^{-sh} A_1 }.
        \]
        Множество собственных чисел
        \[
        \Lambda = \set{ s: f(s) = 0 }
        \]
        называют <i>спектром</i> этой системы.
      </div>

      <p>
        Следующая теорема демонстрирует, что экспоненциальная устойчивость системы
        зависит от расположения её спектра.
      </p>

      <div class="theorem">
        Система (2.1) экспоненциально устойчива тогда и только тогда, когда спектр
        этой системы расположен в левой полуплоскости комплексной плоскости, то есть
        \[
        \forall s_0 \in \Lambda
        \qquad
        \Re(s_0) \lt 0.
        \]
      </div>

      <p>
        Следующая теорема является достаточным условием экспоненциальной устойчивости
        системы (2.1).
      </p>

      <div class="theorem">
        Система (2.1) экспоненциально устойчива, если существует функционал
        \[
        v: PC([-h, 0], \R^n) \to \R
        \]
        такой, что
        <ol>
          <li>
            найдутся $\alpha_1, \alpha_2 \gt 0$ такие, что для всех
            $\varphi \in PC([-h, 0], \R^n)$ выполнены неравенства
            \[
            \alpha_1 \norm{\varphi(0)}^2
            \leqslant v(\varphi)
            \leqslant
            \alpha_2 \norm{\varphi}_h^2;
            \]
          </li>

          <li>
            найдётся такое $\beta \gt 0$, что неравенство
            \[
            \dv{}{t} v(x_t) \leqslant -\beta \norm{x(t)}^2, \qquad t \geqslant 0
            \]
            справедливо вдоль всех решений системы.
          </li>
        </ol>
      </div>

      <h3>Постановка проблемы</h3>
      <div class="problem">
        Пусть система (2.1) экспоненциально устойчива. Требуется для заданной квадратичной
        формы
        \[
        w(x) = x^T W x
        \]
        найти функционал
        \[
        v_0(\varphi): PC([-h, 0], \R^n) \to \R
        \]
        такой, что вдоль всех решений системы справедливо равенство
        \[
        \tag{2.5}
        \dv{}{t} v_0(x_t) = - x^T(t) W x(t), \qquad t \geqslant 0.
        \]
      </div>

      <h3>Случай без запаздывания</h3>
      <p>
        Рассмотрим экспоненциально устойчивую систему
        \[
        \tag{2.6}
        \dv{x}{t} = A x.
        \]
        Через $x(t, x_0)$ будем обозначать решение системы, отвечающее начальному условию
        \[
        x(0, x_0) = x_0.
        \]
        Если начальное условие не принципиально, то будем пользоваться обозначением
        $x(t)$.
      </p>

      <p>
        Будем для заданной квадратичной формы
        \[
        w(x) = x^T W x
        \]
        искать функцию Ляпунова $v(x)$ такую, что
        \[
        \tag{2.7}
        \at{\dv{}{t} v(x)}{(2.6)} = - w(x).
        \]
        Из курса теории управления известно, что эта функция Ляпунова также будет являться
        квадратичной формой, то есть $v(x) = x^T V x$. Эта функция удовлетворяет
        уравнению (2.7), если матрица $V$ является решением матричного уравнения
        Ляпунова
        \[
        \tag{2.8}
        A^T V + V A = -W.
        \]
        Простота построения функции Ляпунова достигается благодаря тому, что нам было
        заранее известно, что оня является квадратичной формой. Предположим теперь, что
        нам это неизвестно (заметим, что именно в такой ситуации мы находимся в случае с
        запаздыванием). Зададимся вопросом: возможно ли определить вид функции Ляпунова
        во время её построения?
      </p>

      <p>
        Для ответа на этот вопрос рассмотрим уравнение (2.7). Заметим, что при подстановке
        в него решения системы (2.6) уравнение принимает вид
        \[
        \dv{}{t} v(x(t)) = -w(x(t)).
        \]
        Ясно, что это уравнение определяет функцию $v(x(t))$ с точностью до константы.
        Чтобы найти эту константу, вспомним, что функция Ляпунова $v(x)$ должна
        равняться нулю при $x = 0$:
        \[
        v(0) = 0.
        \]
        Тогда после интегрирования от $0$ до $T \gt 0$ получаем, что
        \[
        v(x(T, x_0)) - v(x_0) = - \int\limits_{0}^{T} x^T(t, x_0) W x(t, x_0) dt.
        \]
        Так как система (2.6) экспоненциально устойчива, имеем
        \[
        x(T, x_0) \limto{T \to \infty} 0
        \implies
        v(x(T, x_0)) \limto{T \to \infty} 0.
        \]
        Отсюда следует, что
        \[
        \tag{2.9}
        v(x_0) = \int\limits_{0}^{\infty} x^T(t, x_0) W x(t, x_0) dt.
        \]
        Несобственный интеграл сходится также в силу экспоненциальной устойчивости.
      </p>

      <p>
        Известно, что
        \[
        x(t, x_0) = e^{At} x_0,
        \]
        поэтому, подставив это представление в (2.9), получаем
        \[
        v(x_0) = x_0^T \paren{
        \int\limits_{0}^{\infty} e^{A^T t} W e^{A t} dt
        } x_0.
        \]
        Отсюда следует, что функция Ляпунова является квадратичной формой
        \[
        v(x) = x^T V x
        \]
        с матрицей
        \[
        \tag{2.10}
        V = \int\limits_{0}^{\infty} e^{A^T t} W e^{A t} dt.
        \]
        Нетрудно проверить, что эта матрица удовлетворяет уравнению (2.8).
      </p>

      <p>
        Подведём итог.
      </p>
      <ol>
        <li>
          Экспоненциальная устойчивость системы позволяет обосновать уравнение (2.9).
        </li>

        <li>
          Явное выражение для решений системы (2.6) позволяет определить форму требуемой
          функции.
        </li>

        <li>
          Для нахождения матрицы $V$ не требуется вычисление несобственного интеграла,
          так как она удовлетворяет уравнению (2.8).
        </li>
      </ol>

      <h3>Вычисление $v_0(\varphi)$</h3>
      Равенство (2.5) определяет функционал $v_0(\varphi)$ с точностью до
      постоянного слагаемого. Из теоремы 2.3 следует, что это слагаемое
      следует выбирать так, чтобы выполнялось равенство
      \[
      v_0(0_h) = 0,
      \]
      где $0_h \in PC([-h, 0], \R^n)$ &mdash; тривиальная начальная функция.

      <p>
        Интегрируя равенство (2.5) на интервале $[0, T]$, получаем
        \[
        v_0(x_T(\varphi)) - v_0(\varphi)
        =
        - \int\limits_{0}^{T} x^T(t, \varphi) W x(t, \varphi) dt.
        \]
        Так как система (2.1) экспоненциально устойчива, то
        \[
        x_T(\varphi) \limto{T \to \infty} 0,
        \]
        поэтому
        \[
        v_0(\varphi) = \int\limits_{0}^{\infty} x^T(t, \varphi) W x(t, \varphi) dt,
        \qquad
        \varphi \in PC([-h, 0], \R^n).
        \]
        Из экспоненциальной устойчивости также следует, что несобственный интеграл
        определён.
      </p>

      <p>
        Заменим теперь $x(t, \varphi)$ в подынтегральном выражении формулой Коши:
        \[
        \begin{aligned}
        v_0(\varphi)
        &=
        \int\limits_{0}^{\infty} \left[
        K(t) \varphi(0)
        +
        \int\limits_{-h}^{0} K(t - h - \theta_1) A_1 \varphi(\theta_1) d\theta_1
        \right]^T W \times \\
        &\phantom{=} \times
        \left[
        K(t) \varphi(0)
        +
        \int\limits_{-h}^{0} K(t - h - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] dt = \\
        &=
        \varphi^T(0) \left[
        \int\limits_{0}^{\infty} K^T(t) W K(t) dt
        \right] \varphi(0) + \\
        &\phantom{=} + 2 \varphi^T(0)
        \int\limits_{0}^{\infty} K^T(t) W \left[
        \int\limits_{-h}^{0} K(t - h - \theta) A_1 \varphi(\theta) d\theta
        \right] dt + \\
        &\phantom{=} + \int\limits_{0}^{\infty} \left[
        \int\limits_{-h}^{0} K(t - h - \theta_1) A_1 \varphi(\theta_1) d\theta_1
        \right]^T W \left[
        \int\limits_{-h}^{0} K(t - h - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] dt.
        \end{aligned}
        \]

        Здесь мы впервые сталкиваемся с матрицей
        \[
        \tag{2.11}
        U(\tau) = \int\limits_{0}^{\infty} K^T(t) W K(t + \tau) dt,
        \]
        которая будет играть ключевую роль в нашем исследовании.
      </p>

      <p>
        Так как столбцы фундаментальной матрицы $K(t)$ являются решениями
        системы (2.1) с некоторыми начальными условиями, то легко проверить,
        что справедлива оценка
        \[
        \tag{2.12}
        \norm{K(t)} \leqslant \gamma e^{-\sigma t}, \qquad t \geqslant 0.
        \]
        Отсюда следует, что матрица $U(\tau)$ определена для всех $\tau \in \R$.
      </p>

      <div class="lemma">
        При $\tau_0 \in \R$ несобственный интеграл (2.11) сходится абсолютно
        и равномерно по $\tau \in [\tau, \infty)$.

        <div class="proof">
          Пусть $\tau_0 \in \R$. Из (2.12) следует, что
          \[
          \norm{K^T(t) W K(t + \tau)} \leqslant \gamma^2 \norm{W} e^{-\sigma (2 t + \tau)},
          \qquad t \geqslant 0.
          \]
          Пусть $\tau \in [\tau_0, \infty)$, тогда
          \[
          \int\limits_{0}^{\infty} \norm{K^T(t) W K(t + \tau)} dt
          \leqslant
          \frac{\gamma^2}{2 \sigma} \norm{W} e^{-\sigma \tau}
          \leqslant
          \frac{\gamma^2}{2 \sigma} \norm{W} e^{-\sigma \tau_0},
          \]
          откуда и следует абсолютная и равномерная сходимость.
        </div>
      </div>

      <p>
        Покажем, что матрица (2.11) позволяет представить функционал
        $v_0(\varphi)$ в форме, более подходящей для последующего
        анализа.
      </p>

      <p>
        Рассмотрим формулу
        \[
        \begin{aligned}
        v_0(\varphi)
        &=
        \varphi^T(0) \left[
        \int\limits_{0}^{\infty} K^T(t) W K(t) dt
        \right] \varphi(0) + \\
        &\phantom{=} + 2 \varphi^T(0)
        \int\limits_{0}^{\infty} K^T(t) W \left[
        \int\limits_{-h}^{0} K(t - h - \theta) A_1 \varphi(\theta) d\theta
        \right] dt + \\
        &\phantom{=} + \int\limits_{0}^{\infty} \left[
        \int\limits_{-h}^{0} K(t - h - \theta_1) A_1 \varphi(\theta_1) d\theta_1
        \right]^T W \left[
        \int\limits_{-h}^{0} K(t - h - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] dt.
        \end{aligned}
        \]
        Введём обозначения:
        \[
        \begin{aligned}
        R_1 &= \varphi^T(0) \left[
        \int\limits_{0}^{\infty} K^T(t) W K(t) dt
        \right] \varphi(0), \\
        R_2 &= 2 \varphi^T(0)
        \int\limits_{0}^{\infty} K^T(t) W \left[
        \int\limits_{-h}^{0} K(t - h - \theta) A_1 \varphi(\theta) d\theta
        \right] dt, \\
        R_3 &= \int\limits_{0}^{\infty} \left[
        \int\limits_{-h}^{0} K(t - h - \theta_1) A_1 \varphi(\theta_1) d\theta_1
        \right]^T W \left[
        \int\limits_{-h}^{0} K(t - h - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] dt.
        \end{aligned}
        \]

        Первое слагаемое можно представить в виде
        \[
        R_1 = \varphi^T(0) U(0) \varphi(0).
        \]
      </p>

      <p>
        Далее, из леммы 2.1 следует, что можно заменить порядок интегрирования
        во втором слагаемом:
        \[
        \begin{aligned}
        R_2
        &=
        2 \varphi^T(0)
        \int\limits_{0}^{\infty} K^T(t) W \left[
        \int\limits_{-h}^{0} K(t - h - \theta) A_1 \varphi(\theta) d\theta
        \right] dt = \\
        &=
        2 \varphi^T(0)
        \int\limits_{-h}^{0} \left[
        \int\limits_{0}^{\infty} K^T(t) W K(t - h - \theta) dt
        \right]
        A_1 \varphi(\theta) d\theta = \\
        &=
        2 \varphi^T(0)
        \int\limits_{-h}^{0} U(-h - \theta) A_1 \varphi(\theta) d\theta.
        \end{aligned}
        \]
      </p>

      <p>
        Рассмотрим теперь третье слагаемое. Введём обозначение
        \[
        J = \int\limits_{0}^{\infty} K^T(t - \tau_1) W K(t - \tau_2) dt,
        \]
        где $\tau_1, \tau_2$ &mdash; некоторые положительные постоянные. Этот интеграл
        может быть представлен в виде
        \[
        \begin{aligned}
        J
        &=
        \int\limits_{0}^{\infty} K^T(t - \tau_1) W K(t - \tau_2) dt\
        = \Big< \xi := t - \tau_1 \Big> = \\
        &=
        \int\limits_{-\tau_1}^{\infty} K^T(\xi) W K(\xi + \tau_1 - \tau_2) d\xi = \\
        &=
        \int\limits_{-\tau_1}^{0} K^T(\xi) W K(\xi + \tau_1 - \tau_2) d\xi
        +
        \int\limits_{0}^{\infty} K^T(\xi) W K(\xi + \tau_1 - \tau_2) d\xi = \\
        &=
        \int\limits_{-\tau_1}^{0} K^T(\xi) W K(\xi + \tau_1 - \tau_2) d\xi
        + U(\tau_1 - \tau_2).
        \end{aligned}
        \]
        Заметим, что
        \[
        K(\xi) = 0_{n \times n} \qquad \forall \xi \in [-\tau_1, 0),
        \]
        поэтому первое слагаемое равняется нулю, и окончательно имеем
        \[
        J = U(\tau_1 - \tau_2).
        \]

        Теперь, пользуясь леммой 2.1 для обоснования смены порядка интегрирования,
        окончательно получаем
        \[
        \begin{aligned}
        R_3
        &=
        \int\limits_{0}^{\infty} \left[
        \int\limits_{-h}^{0} K(t - h - \theta_1) A_1 \varphi(\theta_1) d\theta_1
        \right]^T W \left[
        \int\limits_{-h}^{0} K(t - h - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] dt = \\
        &=
        \int\limits_{-h}^{0} \varphi^T(\theta_1) A_1^T \left[
        \int\limits_{-h}^{0} \left(
        \int\limits_{0}^{\infty} K^T(t - h - \theta_1) W K(t - h - \theta_2) dt
        \right) A_1 \varphi(\theta_2) d\theta_2
        \right] d\theta_1 = \\
        &=
        \int\limits_{-h}^{0} \varphi^T(\theta_1) A_1^T \left[
        \int\limits_{-h}^{0} U(\theta_1 - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] d\theta_1.
        \end{aligned}
        \]
      </p>

      <p>
        Итак, эти преобразования приводят нас к следующему виду функционала $v_0(\varphi)$:
        \[
        \tag{2.13}
        \begin{aligned}
        v_0(\varphi)
        &=
        \varphi^T(0) U(0) \varphi(0)
        +
        2 \varphi^T(0) \int\limits_{-h}^{0} U(-h - \theta) A_1 \varphi(\theta) d\theta + \\
        &\phantom{=}
        + \int\limits_{-h}^{0} \varphi^T(\theta_1) A_1^T \left[
        \int\limits_{-h}^{0} U(\theta_1 - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] d \theta_1.
        \end{aligned}
        \]

        Заметим, что правая часть зависит от матрицы $U(\tau)$. Это первая (но не последняя)
        причина назвать эту матрицу матрицей Ляпунова системы (2.1).
      </p>

      <div class="definition">
        Матрица
        \[
        U(\tau) = \int\limits_{0}^{\infty} K^T(t) W K(t + \tau) dt
        \]
        называется <i>матрицей Ляпунова</i> системы (2.1) для матрицы $W$.
      </div>

      <div class="lemma">
        Матрица Ляпунова непрерывно зависит от $\tau \geqslant 0$.

        <div class="proof">
          Это утверждение прямо следует из леммы 2.1 и того факта, что $K(t)$
          непрерывна при $t \geqslant 0$.
        </div>
      </div>

      <h3>Матрица Ляпунова: основные свойства</h3>
      <div class="lemma">
        <i>(динамическое свойство)</i>.
        <br />
        Матрица Ляпунова $U(\tau)$ удовлетворяет матричному уравнению
        \[
        \tag{2.14}
        \dv{}{\tau} U(\tau) = U(\tau) A_0 + U(\tau - h) A_1, \qquad \tau \geqslant 0.
        \]

        <div class="proof">
          Пусть $t \geqslant 0$ и $\tau \gt 0$, тогда из того факта, что
          \[
          \dv{}{t} K(t) = K(t) A_0 + K(t - h) A_1, \qquad t \geqslant 0,
          \]
          следует
          \[
          \pd{}{\tau} \left[
          K^T(t) W K(t + \tau)
          \right]
          =
          K^T(t) W \left[
          K(t + \tau) A_0 + K(t + \tau - h) A_1
          \right].
          \]
          Из экспоненциальной устойчивости системы (2.1) следует, что
          \[
          \begin{aligned}
          \norm{
          \pd{}{\tau} \left[
          K^T(t) W K(t + \tau)
          \right]
          }
          &\leqslant
          \norm{K(t)} \norm{W} \norm{K(t + \tau)} \norm{A_0} + \\
          &\phantom{\leqslant}
          +
          \norm{K(t)} \norm{W} \norm{K(t + \tau - h)} \norm{A_1} \leqslant \\
          &\leqslant
          \gamma^2 e^{-\sigma (2 t + \tau)} \norm{W} \paren{
          \norm{A_0} + e^{\sigma h} \norm{A_1}
          } \leqslant \\
          &\leqslant
          \gamma^2 e^{\sigma h} e^{-2 \sigma t} \norm{W} \paren{
          \norm{A_0} + \norm{A_1}
          }.
          \end{aligned}
          \]

          <p>
            С одной стороны, так как интеграл
            \[
            \int\limits_{0}^{\infty}
            \gamma^2 e^{\sigma h} e^{-2 \sigma t} \norm{W} \paren{
            \norm{A_0} + \norm{A_1}
            } dt
            \]
            сходится, то интеграл
            \[
            \int\limits_{0}^{\infty}
            \pd{}{\tau} \left[
            K^T(t) W K(t + \tau)
            \right] dt
            \]
            сходится абсолютно и равномерно по $\tau \geqslant 0$, откуда в свою
            очередь следует, что операции интегрирования и дифференцирования
            перестановочны, поэтому выполняется равенство
            \[
            \begin{aligned}
            \int\limits_{0}^{\infty}
            \pd{}{\tau} \left[
            K^T(t) W K(t + \tau)
            \right] dt
            &=
            \dv{}{\tau} \paren{
            \int\limits_{0}^{\infty} K^T(t) W K(t + \tau) dt
            } = \\
            &=
            \dv{U(\tau)}{\tau}.
            \end{aligned}
            \]

            <div class="todo">
              проверь слова про перестановочность на строгость формулировки.
            </div>
          </p>

          <p>
            С другой стороны,
            \[
            \begin{aligned}
            \int\limits_{0}^{\infty}
            \pd{}{\tau} \left[
            K^T(t) W K(t + \tau)
            \right] dt
            &=
            \paren{
            \int\limits_{0}^{\infty} K^T(t) W K(t + \tau) dt
            } A_0 + \\
            &\phantom{=}
            +
            \paren{
            \int\limits_{0}^{\infty} K^T(t) W K(t + \tau - h) dt
            } A_1 = \\
            &=
            U(\tau) A_0 + U(\tau - h) A_1,
            \end{aligned}
            \]
            откуда и следует выполнение равенства
            \[
            \dv{U(\tau)}{\tau}
            =
            U(\tau) A_0 + U(\tau - h) A_1.
            \]
          </p>
        </div>
      </div>

      <div class="lemma">
        <i>(свойство симметричности)</i>.
        <br />
        Матрица Ляпунова удовлетворяет равенству
        \[
        \tag{2.15}
        U(-\tau) = U^T(\tau), \qquad \tau \geqslant 0.
        \]

        <div class="proof">
          Преобразуем матрицу $U(-\tau)$:
          \[
          \begin{aligned}
          U(-\tau)
          &=
          \int\limits_{0}^{\infty} K^T(t) W K(t - \tau) dt
          = \Big< \xi = t - \tau \Big> = \\
          &=
          \int\limits_{-\tau}^{\infty} K^T(\xi + \tau) W K(\xi) d\xi = \\
          &=
          \int\limits_{-\tau}^{0} K^T(\xi + \tau) W K(\xi) d\xi +
          \int\limits_{0}^{\infty} K^T(\xi + \tau) W K(\xi) d\xi = \\
          &=
          \int\limits_{-\tau}^{0} K^T(\xi + \tau) W K(\xi) d\xi + U^T(\tau).
          \end{aligned}
          \]

          Для фундаментальной матрицы справедливо
          \[
          K(\xi) = 0_{n \times n}, \qquad \xi \in [-\tau, 0),
          \]
          поэтому интеграл в правой части равен нулю, откуда и следует равенство
          \[
          U(-\tau) = U^T(\tau).
          \]
        </div>
      </div>

      <div class="corollary">
        Из уравнения (2.15) следует, что матрица $U(0)$ симметричная:
        \[
        U^T(0) = U(0).
        \]
      </div>

      <div class="corollary">
        Матрица Ляпунова (2.11) бесконечно дифференцируема при $\tau \in (0, h)$.

        <div class="proof">
          Действительно, из свойства симметричности (2.15)
          \[
          U(-\tau) = U^T(\tau), \qquad \tau \geqslant 0
          \]
          и леммы 2.2 следует, что начальное условие для матрицы Ляпунова
          $U(\tau)$ как для решения уравнения (2.14)
          \[
          \dv{}{\tau} U(\tau) = U(\tau) A_0 + U(\tau - h) A_1, \qquad \tau \geqslant 0
          \]
          должно быть непрерывным, откуда следует непрерывная дифференцируемость
          матрицы Ляпунова. Но из этого следует, что начальное условие
          непрерывно дифференцируемо, что влечёт за собой дважды непрерывную
          дифференцируемость матрицы Ляпунова. Очевидно, что этот процесс можно
          продолжать до бесконечности.
        </div>
      </div>

      <div class="lemma">
        <i>(алгебраическое свойство)</i>.
        <br />
        Матрица Ляпунова удовлетворяет матричному уравнению
        \[
        \tag{2.16}
        U(0) A_0 + U(-h) A_1 + A_0^T U(0) + A_1^T U(h) = -W.
        \]

        <div class="proof">
          Рассмотрим производную
          \[
          \begin{aligned}
          \dv{}{t} \left[
          K^T(t) W K(t)
          \right]
          &=
          \left[
          K(t) A_0 + K(t - h) A_1
          \right]^T W K(t) + \\
          &\phantom{=}
          + K^T(t) W \left[
          K(t) A_0 + K(t - h) A_1
          \right], & t \geqslant 0.
          \end{aligned}
          \]
          Проинтегрируем обе части этого уравнения от $t = 0$ до $t = \infty$:
          \[
          \begin{aligned}
          -W
          &=
          \int\limits_{0}^{\infty}
          \left[
          K(t) A_0 + K(t - h) A_1
          \right]^T W K(t) dt + \\
          &\phantom{=}
          +
          \int\limits_{0}^{\infty} K^T(t) W \left[
          K(t) A_0 + K(t - h) A_1
          \right] dt = \\
          &=
          A_0^T U(0) + A_1^T U(h) + U(0) A_0 + U(-h) A_1.
          \end{aligned}
          \]
        </div>
      </div>

      <div class="remark">
        Из свойства симметричности следует, что первая производная матрицы Ляпунова
        терпит разрыв в точке $\tau = 0$.
      </div>

      <div class="lemma">
        Алгебраическое свойство может быть записано в виде
        \[
        U'(+0) - U'(-0) = -W.
        \]
        Здесь $U'(+0)$ и $U'(-0)$ означают производную справа и слева матрицы
        Ляпунова в точке $\tau = 0$.

        <div class="proof">
          Для начала отметим, что
          \[
          U'(+0)
          \bydef=
          \lim\limits_{\tau \to +0} \dv{U(\tau)}{\tau}
          =
          U(0) A_0 + U(-h) A_1.
          \]
          Дифференцируя свойство симметричности, получаем, что
          \[
          \dv{U(-\tau)}{\tau}
          =
          \left[
          \dv{U(\tau)}{\tau}
          \right]^T,
          \qquad \tau \gt 0,
          \]
          поэтому
          \[
          \lim\limits_{\tau \to -0} \dv{U(\tau)}{\tau}
          =
          -
          \left[
          U(0) A_0 + U(-h) A_1
          \right]^T.
          \]
          Но в таком случае
          \[
          \begin{aligned}
          U'(+0) - U'(-0)
          &=
          \lim\limits_{\tau \to +0} \dv{U(\tau)}{\tau}
          -
          \lim\limits_{\tau \to -0} \dv{U(\tau)}{\tau} = \\
          &=
          U(0) A_0 + U(-h) A_1 +
          \left[
          U(0) A_0 + U(-h) A_1
          \right]^T = \\
          &=
          U(0) A_0 + U(-h) A_1
          + A_0^T U(0) + A_1^T U(h) = \\
          &= -W.
          \end{aligned}
          \]
        </div>
      </div>

      <p>
        Несмотря на то, что функционал (2.13)
        \[
        \begin{aligned}
        v_0(\varphi)
        &=
        \varphi^T(0) U(0) \varphi(0)
        +
        2 \varphi^T(0) \int\limits_{-h}^{0} U(-h - \theta) A_1 \varphi(\theta) d\theta + \\
        &\phantom{=}
        + \int\limits_{-h}^{0} \varphi^T(\theta_1) A_1^T \left[
        \int\limits_{-h}^{0} U(\theta_1 - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] d \theta_1
        \end{aligned}
        \]
        был получен из уравнения (2.5)
        \[
        \dv{}{t} v_0(x_t) = - x^T(t) W x(t), \qquad t \geqslant 0,
        \]
        полезно будет явно продемонстрировать, что функционал удовлетворяет
        этому уравнению.
      </p>

      <div class="theorem">
        Функционал (2.13)
        \[
        \begin{aligned}
        v_0(\varphi)
        &=
        \varphi^T(0) U(0) \varphi(0)
        +
        2 \varphi^T(0) \int\limits_{-h}^{0} U(-h - \theta) A_1 \varphi(\theta) d\theta + \\
        &\phantom{=}
        + \int\limits_{-h}^{0} \varphi^T(\theta_1) A_1^T \left[
        \int\limits_{-h}^{0} U(\theta_1 - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] d \theta_1
        \end{aligned}
        \]
        с матрицей Ляпунова (2.11)
        \[
        U(\tau) = \int\limits_{0}^{\infty} K^T(t) W K(t + \tau) dt
        \]
        удовлетворяет уравнению (2.5)
        \[
        \dv{}{t} v_0(x_t) = - x^T(t) W x(t), \qquad t \geqslant 0.
        \]

        <div class="proof">
          Пусть $x(t)$ &mdash; решение системы (2.1). Тогда
          \[
          \begin{aligned}
          v_0(x_t)
          &=
          x_t^T(0) U(0) x_t(0)
          +
          2 x_t^T(0) \int\limits_{-h}^{0} U(-h - \theta) A_1 x_t(\theta) d\theta + \\
          &\phantom{=}
          + \int\limits_{-h}^{0} x_t^T(\theta_1) A_1^T \left[
          \int\limits_{-h}^{0} U(\theta_1 - \theta_2) A_1 x_t(\theta_2) d\theta_2
          \right] d \theta_1 = \\
          &=
          x^T(t) U(0) x(t)
          +
          2 x^T(t) \int\limits_{-h}^{0} U(-h - \theta) A_1 x(t + \theta) d\theta + \\
          &\phantom{=}
          + \int\limits_{-h}^{0} x^T(t + \theta_1) A_1^T \left[
          \int\limits_{-h}^{0} U(\theta_1 - \theta_2) A_1 x(t + \theta_2) d\theta_2
          \right] d \theta_1.
          \end{aligned}
          \]
          Введём обозначения для слагаемых:
          \[
          \begin{aligned}
          R_1(t) &= x^T(t) U(0) x(t), \\
          R_2(t) &=
          2 x^T(t) \int\limits_{-h}^{0} U(-h - \theta) A_1 x(t + \theta) d\theta \\
          R_3(t) &=
          \int\limits_{-h}^{0} x^T(t + \theta_1) A_1^T \left[
          \int\limits_{-h}^{0} U(\theta_1 - \theta_2) A_1 x(t + \theta_2) d\theta_2
          \right] d \theta_1.
          \end{aligned}
          \]
          Продифференцируем каждое из них.

          <ol>
            <li>
              Нетрудно видеть, что
              \[
              \begin{aligned}
              \dv{}{t} R_1(t)
              &=
              2 x^T(t) U(0) \left[
              A_0 x(t) + A_1 x(t - h)
              \right] = \\
              &=
              x^T(t) \left[
              U(0) A_0 + A_0^T U(0)
              \right] x(t)
              +
              2 x^T(t) U(0) A_1 x(t - h).
              \end{aligned}
              \]
            </li>

            <li>
              Проведём замену переменной: $\xi = t + \theta$, тогда
              \[
              R_2(t) = 2 x^T(t) \int\limits_{t - h}^{t} U(-h - \xi + t) A_1 x(\xi) d\xi.
              \]
              Дифференцируем:
              \[
              \begin{aligned}
              \dv{}{t} R_2(t)
              &=
              2 \left[
              A_0 x(t) + A_1 x(t - h)
              \right]^T
              \int\limits_{t - h}^{t} U(-h - \xi + t) A_1 x(\xi) d\xi + \\
              &\phantom{=}
              + 2 x^T(t) U(-h) A_1 x(t) - 2 x^T(t) U(0) A_1 x(t - h) + \\
              &\phantom{=}
              + 2 x^T(t) \int\limits_{t - h}^{t} \paren{
              \pd{}{t} U(-h - \xi + t)
              } A_1 x(\xi) d\xi = \\
              &=
              2 x^T(t) A_0^T \int\limits_{t - h}^{t} U(-h - \xi + t) A_1 x(\xi) d\xi + \\
              &\phantom{=}
              + 2 x^T(t - h) A_1^T \int\limits_{t - h}^{t} U(-h - \xi + t) A_1 x(\xi) d\xi + \\
              &\phantom{=}
              + 2 x^T(t) \int\limits_{t - h}^{t} \paren{
              \pd{}{t} U(-h - \xi + t)
              } A_1 x(\xi) d\xi = \\
              &\phantom{=}
              + x^T(t) \left[ U(-h) A_1 + A_1^T U(h) \right] x(t) - \\
              &\phantom{=}
              - 2 x^T(t) U(0) A_1 x(t - h).
              \end{aligned}
              \]
            </li>

            <li>
              Применяя аналогичные замены переменных к третьему слагаемому, получаем
              \[
              R_3(t) = \int\limits_{t-h}^{t} x^T(\xi_1) A_1^T \left[
              \int\limits_{t-h}^{t} U(\xi_1 - \xi_2) A_1 x(\xi_2) d\xi_2
              \right] d\xi_1.
              \]

              Дифференцируем:
              \[
              \begin{aligned}
              \dv{}{t} R_3(t)
              &=
              x^T(t) A_1^T \int\limits_{t-h}^{t} U(t - \xi) A_1 x(\xi) d\xi - \\
              &\phantom{=}
              - x^T(t - h) A_1^T \int\limits_{t-h}^{t} U(t - h - \xi) A_1 x(\xi) d\xi + \\
              &\phantom{=}
              + \paren{
              \int\limits_{t-h}^{t} x^T(\xi) A_1^T U(\xi - t) d\xi
              } A_1 x(t) - \\
              &\phantom{=}
              - \paren{
              \int\limits_{t-h}^{t} x^T(\xi) A_1^T U(\xi - t + h) d\xi
              } A_1 x(t - h).
              \end{aligned}
              \]

              Рассмотрим два последних слагаемых:
              \[
              \begin{aligned}
              J_1(t)
              &=
              \paren{
              \int\limits_{t-h}^{t} x^T(\xi) A_1^T U(\xi - t) d\xi
              } A_1 x(t) = \\
              &=
              x^T(t) A_1^T
              \int\limits_{t-h}^{t} U(t - \xi) A_1 x(\xi) d\xi, \\
              J_2(t)
              &=
              \paren{
              \int\limits_{t-h}^{t} x^T(\xi) A_1^T U(\xi - t + h) d\xi
              } A_1 x(t - h) = \\
              &=
              x^T(t - h) A_1^T
              \int\limits_{t-h}^{t} U(t - h - \xi) A_1 x(\xi) d\xi.
              \end{aligned}
              \]
              Отсюда следует, что
              \[
              \begin{aligned}
              \dv{}{t} R_3(t)
              &=
              2 x^T(t) A_1^T \int\limits_{t-h}^{t} U(t - \xi) A_1 x(\xi) d\xi - \\
              &\phantom{=}
              - 2 x^T(t - h) A_1^T \int\limits_{t-h}^{t} U(t - h - \xi) A_1 x(\xi) d\xi.
              \end{aligned}
              \]
            </li>
          </ol>

          Итак, имеем
          \[
          \begin{aligned}
          \dv{}{t} R_1(t) + \dv{}{t} R_2(t) + \dv{}{t} R_3(t)
          &=
          x^T(t) \left[
          U(0) A_0 + A_0^T U(0)
          \right] x(t) + \\
          &\phantom{=}
          + \cancel{ 2 x^T(t) U(0) A_1 x(t - h) } + \\
          &\phantom{=}
          + 2 x^T(t) A_0^T \int\limits_{t - h}^{t} U(-h - \xi + t) A_1 x(\xi) d\xi + \\
          &\phantom{=}
          + \cancel{
          2 x^T(t - h) A_1^T \int\limits_{t - h}^{t} U(-h - \xi + t) A_1 x(\xi) d\xi
          } + \\
          &\phantom{=}
          + 2 x^T(t) \int\limits_{t - h}^{t} \paren{
          \pd{}{t} U(-h - \xi + t)
          } A_1 x(\xi) d\xi = \\
          &\phantom{=}
          + x^T(t) \left[ U(-h) A_1 + A_1^T U(h) \right] x(t) - \\
          &\phantom{=}
          - \cancel{ 2 x^T(t) U(0) A_1 x(t - h) } + \\
          &\phantom{=}
          + 2 x^T(t) A_1^T \int\limits_{t-h}^{t} U(t - \xi) A_1 x(\xi) d\xi - \\
          &\phantom{=}
          - \cancel{
          2 x^T(t - h) A_1^T \int\limits_{t-h}^{t} U(t - h - \xi) A_1 x(\xi) d\xi
          } = \\
          &=
          x^T(t) \left[
          U(0) A_0 + A_0^T U(0) + U(-h) A_1 + A_1^T U(h)
          \right] x(t) + \\
          &\phantom{=}
          +
          2 x^T(t) \int\limits_{t-h}^{t} \left[
          A_0^T U(-h - \xi + t) + A_1^T U(t - \xi) + \pd{}{t} U(-h - \xi + t)
          \right] A_1 x(\xi) d\xi.
          \end{aligned}
          \]

          Рассмотрим слагаемое с интегральным сомножителем:
          \[
          S_1(t) = 2 x^T(t) \int\limits_{t-h}^{t} \left[
          A_0^T U(-h - \xi + t) + A_1^T U(t - \xi) + \pd{}{t} U(-h - \xi + t)
          \right] A_1 x(\xi) d\xi.
          \]
          Применяя свойство симметричности (2.15), получаем
          \[
          \begin{aligned}
          \pd{}{t} U(-h - \xi + t)
          &=
          \left[ \pd{}{t} U(h + \xi - t) \right]^T = \\
          &=
          - \left[
          \at{\paren{ \dv{}{\tau} U(\tau) }}{\tau = h + \xi - t}
          \right]^T.
          \end{aligned}
          \]
          Так как $\xi \in [t - h, t]$, то
          \[
          \tau = h + \xi - t \geqslant 0,
          \]
          поэтому можно применить динамическое свойство (2.14), а затем свойство
          симметричности (2.15) матрицы Ляпунова, окончательно получая, что
          \[
          \pd{}{t} U(-h - \xi + t)
          =
          - A_0^T U(-h - \xi + t) - A_1^T U(-\xi + t).
          \]
          Подставляя это выражение в $S_1(t)$, приходим к выводу, что это слагаемое
          равняется нулю.

          <p>
            Итак,
            \[
            \dv{}{t} v_0(x_t)
            =
            x^T(t) \left[
            U(0) A_0 + A_0^T U(0) + U(-h) A_1 + A_1^T U(h)
            \right] x(t).
            \]
            Пользуясь алгебраическим свойством (2.16), окончательно приходим к выводу, что
            \[
            \dv{}{t} v_0(x_t) = - x^T(t) W x(t), \qquad t \geqslant 0.
            \]
          </p>
        </div>
      </div>

      <h3>Матрица Ляпунова: предельный случай</h3>
      <p>
        Рассмотрим, как ведёт себя матрица Ляпунова системы (2.1) при отсутствии
        запаздывания. Это может произойти в двух случаях.
      </p>

      <ol>
        <li>
          Матрица $A_1$ стремится к $0_{n \times n}$, то есть система (2.1) стремится к
          \[
          \dv{x(t)}{t} = A_0 x(t).
          \]
          Будем предполагать, что матрица $A_1$ стремится к $0_{n \times n}$ так, что
          система остаётся экспоненциально устойчивой. Тогда из (2.13)
          \[
          \begin{aligned}
          v_0(\varphi)
          &=
          \varphi^T(0) U(0) \varphi(0)
          +
          2 \varphi^T(0) \int\limits_{-h}^{0} U(-h - \theta) A_1 \varphi(\theta) d\theta + \\
          &\phantom{=}
          + \int\limits_{-h}^{0} \varphi^T(\theta_1) A_1^T \left[
          \int\limits_{-h}^{0} U(\theta_1 - \theta_2) A_1 \varphi(\theta_2) d\theta_2
          \right] d \theta_1
          \end{aligned}
          \]
          следует, что функционал $v_0(x_t)$ стремится к квадратичной форме
          $x^T(t) U(0) x(t)$. Из свойства симметричности (2.15) следует, что
          матрица $U(0)$ симметричная, а алгебраическое свойство (2.16)
          \[
          U(0) A_0 + U(-h) A_1 + A_0^T U(0) + A_1^T U(h) = -W
          \]
          превращается в классическое матричное уравнение Ляпунова для $U(0)$:
          \[
          U(0) A_0 + A_0^T U(0) = - W.
          \]
        </li>

        <li>
          Запаздывание стремится к нулю: $h \to +0$. В этом случае система
          стремится к
          \[
          \dv{x(t)}{t} = (A_0 + A_1) x(t).
          \]
          Снова предположим, что $h \to +0$ таким образом, что система остаётся
          экспоненциально устойчивой. Тогда функционал (2.13) стремится к
          квадратичной форме $x^T(t) U(0) x(t)$. Из свойства симметричности
          следует, что матрица $U(0)$ симметричная, а алгебраическое свойство
          превращается в классическое матричное уравнение Ляпунова
          \[
          U(0) \left[ A_0 + A_1 \right] + \left[ A_0 + A_1 \right]^T U(0) = -W.
          \]
        </li>
      </ol>

      <p>
        Этот краткий анализ даёт ещё одно основание для того, чтобы называть матрицу
        $U(\tau)$ матрицей Ляпунова.
      </p>

      <h3>Матрица Ляпунова: новое определение</h3>
      <p>
        У определения матрицы Ляпунова (2.11)
        \[
        U(\tau) = \int\limits_{0}^{\infty} K^T(t) W K(t + \tau) dt
        \]
        есть два серьёзных недостатка.
      </p>

      <ol>
        <li>
          Это определение применимо только для экспоненциально устойчивых
          систем.
        </li>

        <li>
          Это определение неконструктивно с вычислительной точки зрения. В самом
          деле, для вычисления матрицы Ляпунова $U(\tau)$ нужно вычислить
          значения фундаментальной матрицы $K(t)$ для всех $t \in [0, \infty)$
          (что само по себе может быть затруднительным), а затем вычислить
          несобственный интеграл (2.11) для различных значений $\tau$.
        </li>
      </ol>

      <p>
        В данном параграфе мы перестаём предполагать экспоненциальную устойчивость
        системы (2.1).
      </p>

      <p>
        Докажем для начала следующий результат.
      </p>

      <div class="theorem">
        Пусть $\widetilde U(\tau)$ &mdash; решение уравнения (2.14)
        \[
        \dv{}{\tau} U(\tau) = U(\tau) A_0 + U(\tau - h) A_1, \qquad \tau \geqslant 0,
        \]
        удовлетворяющее уравнениям (2.15)
        \[
        U(-\tau) = U^T(\tau), \qquad \tau \geqslant 0
        \]
        и (2.16)
        \[
        U(0) A_0 + U(-h) A_1 + A_0^T U(0) + A_1^T U(h) = -W.
        \]
        Тогда функционал $\widetilde v_0(\varphi)$ (определяемый формулой (2.13) при
        $U(\tau) = \widetilde U(\tau)$):
        \[
        \begin{aligned}
        \widetilde v_0(\varphi)
        &=
        \varphi^T(0) \widetilde U(0) \varphi(0)
        +
        2 \varphi^T(0) \int\limits_{-h}^{0} \widetilde U(-h - \theta) A_1 \varphi(\theta) d\theta + \\
        &\phantom{=}
        + \int\limits_{-h}^{0} \varphi^T(\theta_1) A_1^T \left[
        \int\limits_{-h}^{0} \widetilde U(\theta_1 - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] d \theta_1
        \end{aligned}
        \]
        удовлетворяет уравнению
        \[
        \dv{}{t} \widetilde v_0(x_t) = - x^T(t) W x(t), \qquad t \geqslant 0
        \]
        вдоль решений системы (2.1).

        <div class="proof">
          Заметим, что при доказательстве теоремы 2.4 были использованы только
          свойства (2.14)-(2.16) матрицы $U(\tau)$. Так как матрица $\widetilde
          U(\tau)$ удовлетворяет этим свойствам, функционал $\widetilde
          v_0(\varphi)$ также удовлетворяет уравнению (2.5).
        </div>
      </div>

      <p>
        Теорема 2.5 даёт основание для следующего определения.
      </p>

      <div class="definition">
        Матрицу $U(\tau)$ называют <i>матрицей Ляпунова</i> системы (2.1)
        для матрицы $W$, если она удовлетворяет свойствам
        <ul>
          <li>
            (2.14):
            \[
            \dv{}{\tau} U(\tau) = U(\tau) A_0 + U(\tau - h) A_1, \qquad \tau \geqslant 0,
            \]
          </li>

          <li>
            (2.15):
            \[
            U(-\tau) = U^T(\tau), \qquad \tau \geqslant 0,
            \]
          </li>

          <li>
            (2.16):
            \[
            U(0) A_0 + U(-h) A_1 + A_0^T U(0) + A_1^T U(h) = -W.
            \]
          </li>
        </ul>
      </div>

      <p>
        С одной стороны, новое определение не требует экспоненциальной
        устойчивости исходной системы (2.1). С другой стороны, возникает
        вопрос: если система (2.1) экспоненциально устойчива, эквивалентно
        ли новое определение старому?
      </p>

      <div class="theorem">
        Пусть система (2.1) экспоненциально устойчива. Тогда матрица (2.11)
        \[
        U(\tau) = \int\limits_{0}^{\infty} K^T(t) W K(t + \tau) dt
        \]
        является единственным решением уравнения (2.14)
        \[
        \dv{}{\tau} U(\tau) = U(\tau) A_0 + U(\tau - h) A_1, \qquad \tau \geqslant 0,
        \]
        удовлетворяющим условиям (2.15) и (2.16).

        <div class="proof">
          В самом деле, матрица (2.11) удовлетворяет условиям (2.14)-(2.16) (см.
          леммы 2.3-2.5).
          <br />
          Проверим единственность. Предположим, что существуют две матрицы
          $U_1(\tau)$ и $U_2(\tau)$, удовлетворяющие свойствам (2.14)-(2.16).
          Для каждой из них определим функционалы $v_0^{(1)}(\varphi)$ и
          $v_0^{(2)}(\varphi)$ по формуле (2.13). Тогда из теоремы 2.5 следует, что
          \[
          \begin{aligned}
          \dv{}{t} v_0^{(1)}(x_t) &= - x^T(t) W x(t), \\
          \dv{}{t} v_0^{(2)}(x_t) &= - x^T(t) W x(t).
          \end{aligned}
          \]
          Так как их разница
          \[
          \Delta v(x_t) = v_0^{(2)}(x_t) - v_0^{(1)}(x_t)
          \]
          удовлетворяет уравнению
          \[
          \dv{}{t} \Delta v(x_t) = 0, \qquad t \geqslant 0,
          \]
          заключаем, что равенство
          \[
          \Delta v(x_t(\varphi)) = \Delta v(\varphi), \qquad t \geqslant 0
          \]
          выполняется вдоль решений системы.

          <p>
            Экспоненциальная устойчивость системы означает, что
            \[
            x_t(\varphi) \limto{t \to \infty} 0_h,
            \]
            поэтому
            \[
            \Delta v(x_t(\varphi)) \limto{t \to \infty} 0.
            \]
            Отсюда следует, что для любой начальной функции $\varphi \in PC([-h, 0], \R^n)$
            выполнено равенство
            \[
            \tag{2.17}
            \begin{aligned}
            0 = \Delta v(\varphi)
            &=
            \varphi^T(0) \Delta U(0) \varphi(0)
            +
            2 \varphi^T(0) \int\limits_{-h}^{0} \Delta U(-h - \theta) A_1 \varphi(\theta) d\theta + \\
            &\phantom{=}
            + \int\limits_{-h}^{0} \varphi^T(\theta_1) A_1^T \left[
            \int\limits_{-h}^{0} \Delta U(\theta_1 - \theta_2) A_1 \varphi(\theta_2) d\theta_2
            \right] d \theta_1,
            \end{aligned}
            \]
            где $\Delta U(\tau) = U_2(\tau) - U_1(\tau)$.
          </p>

          <ul>
            <li>
              Выберем произвольный вектор $\gamma \in \R^n$ и рассмотрим начальную
              функцию
              \[
              \varphi^{(1)}(\theta) =
              \begin{cases}
              0, & \theta \in [-h, 0), \\
              \gamma, & \theta = 0.
              \end{cases}
              \]
              Для этой начальной функции уравнение (2.17) имеет вид
              \[
              0 = \Delta v (\varphi^{(1)}) = \gamma^T \Delta U(0) \gamma.
              \]
              В силу произвольности вектора $\gamma$ и симметричности матрицы $\Delta U(0)$
              имеем
              \[
              \tag{2.18}
              \Delta U(0) = 0_{n \times n}.
              \]
            </li>

            <li>
              Рассмотрим произвольные векторы $\gamma, \mu \in \R^n$ и $\tau \in (0, h]$.
              Выберем $\varepsilon \gt 0$ так, чтобы
              \[
              -\tau + \varepsilon \lt 0.
              \]
              Рассмотрим начальную функцию
              \[
              \varphi^{(2)}(\theta) =
              \begin{cases}
              \mu, & \theta \in [-\tau, -\tau + \varepsilon), \\
              \gamma, & \theta = 0, \\
              0 & \text{для всех остальных точек отрезка } [-h, 0].
              \end{cases}
              \]
              Нетрудно проверить, что
              \[
              \begin{aligned}
              \Delta v(\varphi^{(2)})
              &=
              2 \int\limits_{-\tau}^{-\tau + \varepsilon}
              \gamma^T \Delta U(-h - \theta) A_1 \mu d\theta + \\
              &\phantom{=}
              + \int\limits_{-\tau}^{-\tau + \varepsilon} \left[
              \int\limits_{-\tau}^{-\tau + \varepsilon}
              \mu^T A_1^T \Delta U(\theta_1 - \theta_2) A_1 \mu d\theta_2
              \right] d\theta_1.
              \end{aligned}
              \]
              Для достаточно малых $\varepsilon$ справедливы равенства
              \[
              \begin{gathered}
              2 \int\limits_{-\tau}^{-\tau + \varepsilon}
              \gamma^T \Delta U(-h - \theta) A_1 \mu d\theta
              =
              2 \varepsilon \gamma^T \Delta U(\tau - h) A_1 \mu + o(\varepsilon), \\
              \int\limits_{-\tau}^{-\tau + \varepsilon} \left[
              \int\limits_{-\tau}^{-\tau + \varepsilon}
              \mu^T A_1^T \Delta U(\theta_1 - \theta_2) A_1 \mu d\theta_2
              \right] d\theta_1
              = o(\varepsilon),
              \end{gathered}
              \]
              поэтому
              \[
              \Delta v(\varphi^{(2)})
              =
              2 \varepsilon \gamma^T \Delta U(\tau - h) A_1 \mu + o(\varepsilon).
              \]
              Так как равенство
              \[
              \Delta v(\varphi^{(2)}) = 0
              \]
              выполняется для любого достаточно малого $\varepsilon \gt 0$, заключаем, что
              \[
              2  \gamma^T \Delta U(\tau - h) A_1 \mu = 0.
              \]
              В силу произвольности векторов $\gamma, \mu$ имеем
              \[
              \tag{2.19}
              \Delta U(\tau - h) A_1 = 0_{n \times n}.
              \]
              Это равенство выполнено для любого $\tau \in (0, h]$. Пользуясь
              непрерывностью матриц Ляпунова, получаем, что равенство (2.19)
              справедливо для всех $\tau \in [0, h]$.

              <p>
                Матрицы $U_1(\tau), U_2(\tau)$ удовлетворяют свойству (2.14), поэтому
                \[
                \dv{}{\tau} \Delta U(\tau) = \Delta U(\tau) A_0 + \Delta U(\tau - h) A_1,
                \qquad \tau \geqslant 0.
                \]
                Пользуясь условием (2.19), получаем
                \[
                \dv{}{\tau} \Delta U(\tau) = \Delta U(\tau) A_0,
                \qquad \tau \in [0, h].
                \]
                Так как матрица $\Delta U(\tau)$ удовлетворяет уравнению (2.18)
                \[
                \Delta U(0) = 0_{n \times n},
                \]
                мы сразу же заключаем, что
                \[
                \Delta U(\tau) = 0_{n \times n}, \qquad \tau \in [0, h].
                \]
                Отсюда окончательно получаем, что $U_1(\tau) = U_2(\tau)$.
              </p>
            </li>
          </ul>
        </div>
      </div>

      <h3>Матрица Ляпунова: вопросы существования и единственности</h3>
      <p>
        Определение 2.5 поднимает вопрос: при каких условиях матрица Ляпунова
        существует? Иными словами, при каких условиях уравнение (2.14)
        \[
        \dv{}{\tau} U(\tau) = U(\tau) A_0 + U(\tau - h) A_1, \qquad \tau \geqslant 0
        \]
        имеет решение, удовлетворяющее свойствам (2.15)
        \[
        U(-\tau) = U^T(\tau), \qquad \tau \geqslant 0
        \]
        и (2.16)
        \[
        U(0) A_0 + U(-h) A_1 + A_0^T U(0) + A_1^T U(h) = -W.
        \]
        Теорема 2.6 даёт частичный ответ на этот вопрос. В данном параграфе мы
        подробнее рассмотрим вопрос существования и единственности матрицы
        Ляпунова.
      </p>

      <p>
        Рассмотрим две вспомогательные матрицы:
        \[
        \tag{2.20}
        Y(\tau) = U(\tau), \qquad Z(\tau) = U(\tau - h), \quad \tau \in [0, h].
        \]
      </p>

      <div class="lemma">
        Рассмотрим матрицу Ляпунова $U(\tau)$ для симметричной матрицы $W$.
        Тогда вспомогательные матрицы (2.20) являются решениями системы матричных уравнений
        \[
        \tag{2.21}
        \begin{aligned}
        \dv{}{\tau} Y(\tau) &= Y(\tau) A_0 + Z(\tau) A_1, \\
        \dv{}{\tau} Z(\tau) &= - A_1^T Y(\tau) - A_0^T Z(\tau)
        \end{aligned}
        \]
        с граничными условиями
        \[
        \tag{2.22}
        Y(0) = Z(h),
        \qquad
        A_0^T Y(0) + Y(0) A_0 + A_1^T Y(h) + Z(0) A_1 = -W.
        \]

        <div class="proof">
          Уравнение
          \[
          \dv{}{\tau} Y(\tau) = Y(\tau) A_0 + Z(\tau) A_1
          \]
          сразу следует из уравнения (2.14).

          <p>
            Заметим, что
            \[
            Z(\tau) = U^T(h - \tau), \qquad \tau \in [0, h],
            \]
            поэтому
            \[
            \begin{aligned}
            \dv{}{\tau} Z(\tau)
            &=
            \left[
            \dv{}{\tau} U(h - \tau)
            \right]^T = \\
            &=
            - \left[
            U(h - \tau) A_0 + U(-\tau) A_1
            \right]^T = \\
            &=
            - A_1^T U(\tau) - A_0^T U(\tau - h) = \\
            &=
            - A_1^T Y(\tau) - A_0^T Z(\tau).
            \end{aligned}
            \]
            Таким образом, приходим к системе (2.21).
          </p>

          <p>
            Рассмотрим теперь граничные условия. Первое из них непосредственно следует из
            определения (2.20) вспомогательных матриц, второе же является алгебраическим
            свойством (2.16), записанным с использованием вспомогательных матриц.
          </p>
        </div>
      </div>

      <p>
        Покажем теперь, что всякое решение системы (2.21) с граничными условиями (2.22)
        порождает матрицу Ляпунова для матрицы $W$.
      </p>

      <div class="theorem">
        Если пара $(Y(\tau), Z(\tau))$ удовлетворяет системе (2.21)
        \[
        \begin{aligned}
        \dv{}{\tau} Y(\tau) &= Y(\tau) A_0 + Z(\tau) A_1, \\
        \dv{}{\tau} Z(\tau) &= - A_1^T Y(\tau) - A_0^T Z(\tau)
        \end{aligned}
        \]
        с граничными условиями
        \[
        Y(0) = Z(h),
        \qquad
        A_0^T Y(0) + Y(0) A_0 + A_1^T Y(h) + Z(0) A_1 = -W,
        \]
        то матрица
        \[
        \tag{2.23}
        U(\tau) = \frac{1}{2} \left[ Y(\tau) + Z^T(h - \tau) \right],
        \qquad \tau \in [0, h]
        \]
        будет являться матрицей Ляпунова для матрицы $W$, если мы расширим её на промежуток
        $[-h, 0)$, положив
        \[
        U(-\tau) = U^T(\tau), \qquad \tau \in (0, h].
        \]

        <div class="proof">
          Проверим, что матрица (2.23) удовлетворяет условиям определения 2.5.

          <ul>
            <li>
              Так как мы явно расширили матрицу $U(\tau)$ на промежуток $[-h, 0)$,
              положив
              \[
              U(-\tau) = U^T(\tau), \qquad \tau \in (0, h],
              \]
              то для проверки свойства симметричности достаточно проверить, что
              матрица
              \[
              U(0) = \frac{1}{2} \left[
              Y(0) + Z^T(h)
              \right]
              \]
              симметрична. Из первого граничного условия следует, что
              $Y(0) = Z(h)$, поэтому
              \[
              U(0) = \frac{1}{2} \left[ Y(0) + Y^T(0) \right],
              \]
              то есть матрица $U(0)$ действительно симметрична.
            </li>

            <li>
              Рассмотрим теперь алгебраическое свойство. Для начала заметим, что
              справедливы следующие матричные уравнения:
              \[
              \begin{aligned}
              U(0) A_0 + A_0^T U(0)
              &=
              \frac{1}{2} \left[
              Y(0) + Y^T(0)
              \right] A_0
              +
              \frac{1}{2} A_0^T \left[
              Y(0) + Y^T(0)
              \right] = \\
              &=
              \frac{1}{2} \left[
              Y(0) A_0 + A_0^T Y(0)
              \right]
              +
              \frac{1}{2} \left[
              Y(0) A_0 + A_0^T Y(0)
              \right]^T, \\
              U(-h) A_1 + A_1^T U(h)
              &=
              \frac{1}{2} \left[
              Y(h) + Z^T(0)
              \right]^T A_1
              +
              \frac{1}{2} A_1^T \left[
              Y(h) + Z^T(0)
              \right] = \\
              &=
              \frac{1}{2} \left[
              Z(0) A_1 + A_1^T Y(h)
              \right]
              +
              \frac{1}{2} \left[
              Z(0) A_1 + A_1^T Y(h)
              \right]^T.
              \end{aligned}
              \]
              Следовательно,
              \[
              \begin{aligned}
              R
              &=
              U(0) A_0 + A_0^T U(0) + U(-h) A_1 + A_1^T U(h) = \\
              &=
              \phantom{+}
              \frac{1}{2} \left[
              Y(0) A_0 + A_0^T Y(0)
              +
              Z(0) A_1 + A_1^T Y(h)
              \right] + \\
              &\phantom{=}
              +
              \frac{1}{2} \left[
              Y(0) A_0 + A_0^T Y(0)
              +
              Z(0) A_1 + A_1^T Y(h)
              \right]^T.
              \end{aligned}
              \]
              Но из второго граничного условия следует, что
              \[
              R = - \frac{1}{2} W - \frac{1}{2} W^T = -W.
              \]
            </li>

            <li>
              Наконец, проверим динамическое свойство. Рассмотрим производную
              матрицы $U(\tau)$:
              \[
              \begin{aligned}
              \dv{}{\tau} U(\tau)
              &=
              \frac{1}{2} \dv{Y(\tau)}{\tau} + \frac{1}{2} \dv{Z^T(h - \tau)}{\tau} = \\
              &=
              \frac{1}{2} \left[
              Y(\tau) A_0 + Z(\tau) A_1
              \right]
              -
              \frac{1}{2} \left[
              - A_1^T Y(h - \tau) - A_0^T Z(h - \tau)
              \right]^T = \\
              &=
              \frac{1}{2} \left[
              Y(\tau) + Z^T(h - \tau)
              \right] A_0
              +
              \frac{1}{2} \left[
              Y(h - \tau) + Z^T(\tau)
              \right]^T A_1 = \\
              &= U(\tau) A_0 + U^T(h - \tau) A_1 = \\
              &= U(\tau) A_0 + U(\tau - h) A_1, \qquad \tau \in [0, h].
              \end{aligned}
              \]
            </li>
          </ul>

          Таким образом, матрица (2.23) удовлетворяет определению 2.5, то есть
          является матрицей Ляпунова для матрицы $W$.
        </div>
      </div>

      <div class="corollary">
        Если система (2.21) с граничными условиями (2.22) имеет единственное решение
        $(Y(\tau), Z(\tau))$, то матрица
        \[
        U(\tau) = Y(\tau), \qquad \tau \in [0, h]
        \]
        является единственной матрицей Ляпунова для матрицы $W$.

        <div class="proof">
          Для начала покажем, что если пара $(Y(\tau), Z(\tau))$ является решением
          системы (2.21) с граничными условиями (2.22), то пара
          \[
          \tag{2.24}
          \paren{ \widetilde Y(\tau), \widetilde Z(\tau) }
          =
          \paren{ Z^T(h - \tau), Y^T(h - \tau) }
          \]
          также является решением этой системы с граничными условиями.

          <p>
            Проверим, что это решение удовлетворяет системе (2.21):
            \[
            \begin{aligned}
            \dv{}{\tau} \widetilde Y(\tau)
            &=
            - \left[
            - A_1^T Y(h - \tau) - A_0^T Z(h - \tau)
            \right]^T
            = \widetilde Y(\tau) A_0 + \widetilde Z(\tau) A_1, \\
            \dv{}{\tau} \widetilde Z(\tau)
            &=
            - \left[
            Y(h - \tau) A_0 + Z(h - \tau) A_1
            \right]^T
            =
            - A_1^T \widetilde Y(\tau) - A_0^T \widetilde Z(\tau).
            \end{aligned}
            \]
            Теперь проверим, что матрицы удовлетворяют граничным условием. Действительно,
            \[
            \widetilde Y(0) - \widetilde Z(h)
            =
            \left[
            Z(h) - Y(0)
            \right]^T
            = 0_{n \times n}.
            \]
            Второе граничное условие:
            \[
            \begin{aligned}
            \widetilde R
            &=
            \widetilde Y(0) A_0 + A_0^T \widetilde Y(0)
            + A_1^T \widetilde Y(h) + \widetilde Z(0) A_1 = \\
            &= Z^T(h) A_0 + A_0^T Z^T(h) + A_1^T Z^T(0) + Y^T(h) A_1 = \\
            &= \left[
            A_0^T Y(0) + Y(0) A_0 + Z(0) A_1 + A_1^T Y(h)
            \right]^T = \\
            &= -W.
            \end{aligned}
            \]
          </p>

          <p>
            Так как по условию теоремы решение системы (2.21) с граничными условиями (2.22)
            единственно, то
            \[
            Y(\tau) = Z^T(h - \tau), \qquad \tau \in [0, h],
            \]
            поэтому
            \[
            U(\tau)
            = \frac{1}{2} \left[ Y(\tau) + Z^T(h - \tau) \right]
            = Y(\tau), \qquad \tau \in [0, h]
            \]
            является матрицей Ляпунова для матрицы $W$.
          </p>
        </div>
      </div>

      <p>
        Рассмотрим следующее очень важное условие для системы (2.1). В случае
        без запаздывания оно хорошо известно, так как оно гарантирует
        существование единственного решения классического матричного уравнения
        Ляпунова для любой матрицы $W$.
      </p>

      <div class="definition">
        Говорят, что система (2.1) <i>удовлетворяет условию Ляпунова</i>, если
        спектр этой системы
        \[
        \Lambda = \left\{
        s \in \C: \; \det\paren{ sI - A_0 - e^{-sh} A_1 } = 0
        \right\}
        \]
        не содержит ни одной точки $s_0$ такой, для которой точка $-s_0$ тоже
        принадлежала бы спектру $\Lambda$.
      </div>

      <div class="remark">
        Если система (2.1) удовлетворяет условию Ляпунова, то ни одно его
        собственное число не лежит на мнимой оси.
      </div>

      <p>
        Докажем вспомогательное утверждение.
      </p>

      <div class="lemma">
        Если система (2.21) с граничными условиями (2.22) имеет решение
        $(Y(\tau), Z(\tau))$ при $W = 0_{n \times n}$, то
        \[
        Y(\tau) = Z(h + \tau), \qquad \tau \in \R.
        \]

        <div class="proof">
          Докажем, что матрицы $Y(\tau)$ и $Z(\tau)$ удовлетворяют матричному
          дифференциальному уравнению 2-го порядка:
          \[
          \tag{2.26}
          \ddv2{X}{\tau} = \dv{X}{\tau} A_0 - A_0^T \dv{X}{\tau} + A_0^T X A_0 - A_1^T X A_1.
          \]
          Для этого продифференцируем первое уравнение системы (2.21):
          \[
          \begin{aligned}
          \ddv2{Y(\tau)}{\tau}
          &=
          \dv{Y(\tau)}{\tau} A_0 + \dv{Z(\tau)}{\tau} A_1 = \\
          &=
          \dv{Y(\tau)}{\tau} A_0 - A_1^T Y(\tau) A_1 - A_0^T Z(\tau) A_1.
          \end{aligned}
          \]
          Первое уравнение системы (2.21) позволяет представить последнее слагаемое в виде
          \[
          - A_0^T Z(\tau) A_1 = - A_0^T \left[
          \dv{Y(\tau)}{\tau} - Y(\tau) A_0
          \right].
          \]
          Окончательно имеем
          \[
          \ddv2{Y(\tau)}{\tau}
          =
          \dv{Y(\tau)}{\tau} A_0 - A_1^T Y(\tau) A_1
          - A_0^T \dv{Y(\tau)}{\tau}
          + A_0^T Y(\tau) A_0,
          \]
          откуда и следует, что $Y(\tau)$ удовлетворяет (2.26).

          <p>
            Аналогичными рассуждениями доказывается, что $Z(\tau)$ является
            решением (2.26).
          </p>

          <p>
            Всякое решение (2.26) единственным образом определяется начальными
            условиями $X(\tau_0)$ и $X'(\tau_0)$. Для $W = 0_{n \times n}$ второе
            граничное условие (2.22) можно представить в виде
            \[
            \begin{aligned}
            0_{n \times n}
            &=
            Y(0) A_0 + Z(0) A_1 + A_0^T Z(h) + A_1^T Y(h) = \\
            &=
            Y'(0) - Z'(h).
            \end{aligned}
            \]
            Если к этому добавить первое граничное условие
            \[
            Y(0) = Z(h),
            \]
            то очевидным образом приходим к равенству (2.25)
            \[
            Y(\tau) = Z(h + \tau), \qquad \tau \in \R.
            \]
          </p>
        </div>
      </div>

      <p>
        Итак, сформулируем основной результат данного параграфа.
      </p>

      <div class="theorem">
        Система (2.1) имеет единственную матрицу Ляпунова для симметричной матрицы $W$
        тогда и только тогда, когда эта система удовлетворяет условию Ляпунова.

        <div class="proof">
          <div class="sufficiency">
            Из теоремы 2.7 известно, что матрица Ляпунова для заданной
            симметричной матрицы $W$ существует, если существует решение
            системы (2.21)
            \[
            \begin{aligned}
            \dv{}{\tau} Y(\tau) &= Y(\tau) A_0 + Z(\tau) A_1, \\
            \dv{}{\tau} Z(\tau) &= - A_1^T Y(\tau) - A_0^T Z(\tau)
            \end{aligned}
            \]
            с граничными условиями
            \[
            Y(0) = Z(h),
            \qquad
            A_0^T Y(0) + Y(0) A_0 + A_1^T Y(h) + Z(0) A_1 = -W.
            \]
            Покажем, что если система (2.1) удовлетворяет условию Ляпунова, то
            система (2.21) с граничными условиями (2.22) имеет единственное
            решение.

            <p>
              Итак, пусть система (2.1) удовлетворяет условию Ляпунова.
            </p>

            <p>
              Заметим, что система (2.21) линейна и стационарна. Для определения
              некоторого решения этой системы необходимо задать начальные условия
              \[
              Y(0) = Y_0, \qquad Z(0) = Z_0.
              \]
              Это означает, что суммарно начальные условия содержат $2n^2$ неизвестных.
            </p>

            <p>
              Граничные условия (2.22) задают систему из $2n^2$ скалярных
              линейных алгебраических уравнений с $2n^2$ неизвестными. Известно,
              что СЛАУ имеет единственное решение тогда и только тогда, когда
              единственное решение системы при $W = 0_{n \times n}$ является
              тривиальным.
            </p>

            <p>
              Допустим, что существует нетривиальное решение $(Y_0, Z_0)$ СЛАУ
              при $W = 0_{n \times n}$. Приняв это решение за начальные условия,
              получаем нетривиальное решение $(Y(\tau), Z(\tau))$, $\tau \in [0,h]$,
              системы (2.21) с граничными условиями (2.22) при $W = 0_{n \times n}$.
              Это решение может быть представлено в виде
              \[
              Y(\tau) = \sum\limits_{\nu=0}^{N} e^{s_\nu \tau} P_\nu(\tau),
              \qquad
              Z(\tau) = \sum\limits_{\nu=0}^{N} e^{s_\nu \tau} Q_\nu(\tau),
              \]
              где $s_\nu$ &mdash; различные собственные значения системы (2.21),
              а $P_\nu(\tau), Q_\nu(\tau)$ &mdash; многочлены с матричными коэффициентами.
              Решение $(Y(\tau), Z(\tau))$ нетривиально, поэтому как минимум один
              из многочленов $P_\nu(\tau)$ (допустим, $P_0(\tau)$) нетривиален,
              так как иначе $Y(\tau) \equiv 0_{n \times n}$, и из равенства (2.25)
              следовало бы, что $Z(\tau) \equiv 0_{n \times n}$.
            </p>

            <p>
              Пусть многочлен $P_0(\tau)$ имеет степень $l$:
              \[
              P_0(\tau) = \sum\limits_{j=0}^{l} \tau^j B_j,
              \]
              где $B_j$ &mdash; постоянные матрицы $n \times n$, причём
              $B_l \neq 0_{n \times n}$. Из леммы 2.8 следует, что $Y(\tau) = Z(h + \tau)$,
              поэтому
              \[
              P_0(\tau) = e^{s_0 h} Q_0(\tau + h).
              \]
              Следовательно, $Q_0(\tau)$ также является невырожденным полиномом степени $l$:
              \[
              Q_0(\tau) = \sum\limits_{j=0}^{l} \tau^j C_j,
              \]
              где $C_l = e^{-s_0 h} B_l$.
            </p>

            <p>
              Учитывая (2.25), представим первое уравнение системы (2.21) как
              \[
              \dv{}{\tau} Y(\tau) = Y(\tau) A_0 + Y(\tau - h) A_1.
              \]
              Отсюда следует, что
              \[
              \begin{aligned}
              0_{n \times n}
              &=
              \sum\limits_{\nu=0}^{N} e^{s_\nu \tau} \left[
              s_\nu P_\nu(\tau) + \dv{P_\nu(\tau)}{\tau}
              \right] - \\
              &\phantom{=}
              - \sum\limits_{\nu=0}^{N} e^{s_\nu \tau} \left[
              P_\nu(\tau) A_0 + e^{-s_\nu h} P_\nu(\tau - h) A_1
              \right].
              \end{aligned}
              \]
              Так как все собственные числа различны, то из этого равенства следует,
              что для каждого $\nu$ справедливо
              \[
              s_\nu P_\nu(\tau) + \dv{P_\nu(\tau)}{\tau}
              - P_\nu(\tau) A_0 - e^{-s_\nu h} P_\nu(\tau - h) A_1 = 0_{n \times n}.
              \]
              Рассмотрим уравнение при $\nu = 0$. Собрав все слагаемые при степени $l$,
              получаем
              \[
              B_l \paren{
              s_0 I - A_0 - e^{-s_0 h} A_1
              } = 0_{n \times n}.
              \]
              Так как $B_l \neq 0_{n \times n}$, то предыдущее равенство справедливо только
              если
              \[
              \det \paren{
              s_0 I - A_0 - e^{-s_0 h} A_1
              } = 0.
              \]
              Это означает, что $s_0$ является собственным числом исходной системы (2.1).
            </p>

            <p>
              Рассмотрим второе уравнение системы (2.21). Из равенства $Y(\tau) = Z(\tau + h)$
              следует, что
              \[
              \dv{}{\tau} Z(\tau) = -A_1^T Z(\tau + h) - A_0^T Z(\tau).
              \]
              Пользуясь аналогичными рассуждениями, получаем новую систему уравнений
              \[
              s_\nu Q_\nu(\tau) + \dv{Q_\nu(\tau)}{\tau} + A_1^T Q_\nu(\tau + h)
              + A_0^T Q_\nu(\tau) = 0_{n \times n},
              \qquad \nu = \overline{0, N}.
              \]
              Собрав при $\nu = 0$ все слагаемые при степени $l$, получаем
              \[
              \left[
              s_0 I + A_0 + e^{s_0 h} A_1
              \right]^T C_l = 0_{n \times n}.
              \]
              Так как $C_l \neq 0_{n \times n}$, то это равенство возможно только при
              \[
              \det
              \left[
              (-s_0) I - A_0 - e^{- (-s_0) h} A_1
              \right] = 0,
              \]
              откуда следует, что $-s_0$ также является собственным числом
              системы (2.1).  Значит, система (2.1) не удовлетворяет условию
              Ляпунова, но это противоречит условию теоремы. Значит, не
              существует нетривиального решения СЛАУ из граничных условий при $W
              = 0_{n \times n}$, поэтому существует единственное решение системы
              (2.21) с условиями (2.22) для любой симметричной матрицы $W$, и
              это решение порождает соответствующую матрицу Ляпунова (см.
              теорему 2.7).
            </p>
          </div>

          <div class="necessity">
            Предположим, что система (2.1) не удовлетворяет условию Ляпунова, то есть
            существуют собственные значения $s_0$ и $-s_0$ системы (2.1). Значит,
            существуют ненулевые векторы $\gamma, \mu \in \C^n$ такие, что
            \[
            \mu^T \left[
            s_0 I - A_0 - e^{-s_0 h} A_1
            \right] = 0,
            \qquad
            \left[
            (-s_0) I - A_0 - e^{-(-s_0) h} A_1
            \right]^T \gamma = 0.
            \]
            Покажем, что в этом случае существует нетривиальное решение
            $(Y(\tau), Z(\tau))$, $\tau \in [0, h]$, системы (2.21) с граничными условиями
            (2.22) при $W = 0_{n \times n}$.

            <p>
              Пусть
              \[
              Y(\tau) = e^{s_0 \tau} \gamma \mu^T,
              \qquad
              Z(\tau) = e^{s_0 (\tau - h)} \gamma \mu^T,
              \]
              тогда
              \[
              \begin{aligned}
              \dv{}{\tau} Y(\tau)
              &=
              s_0 e^{s_0 \tau} \gamma \mu^T = \\
              &=
              e^{s_0 \gamma \mu^T} \paren{ A_0 + e^{-s_0 h} A_1 } = \\
              &=
              Y(\tau) A_0 + Z(\tau) A_1, \\
              \dv{}{\tau} Z(\tau)
              &=
              s_0 e^{s_0 (\tau - h)} \gamma \mu^T = \\
              &=
              e^{s_0 (\tau - h)} \paren{
              - A_0^T - e^{s_0 h} A_1^T
              } \gamma \mu^T = \\
              &=
              -A_1^T Y(\tau) - A_0^T Z(\tau).
              \end{aligned}
              \]
              Так как $Y(\tau) = Z(\tau + h)$, то
              \[
              Y(0) = Z(h),
              \qquad
              \at{\dv{}{\tau} Y(\tau)}{\tau = 0}
              =
              \at{\dv{}{\tau} Z(\tau)}{\tau = h}.
              \]
              Из теоремы 2.7 следует, что ненулевое решение определяет матрицу Ляпунова
              для матрицы $W = 0_{n \times n}$:
              \[
              U_0(\tau)
              =
              \frac{1}{2} \left[
              Y(\tau) + Z^T(h - \tau)
              \right]
              =
              \frac{1}{2} \left[
              e^{s_0 \tau} \gamma \mu^T
              + e^{-s_0 \tau} \mu \gamma^T
              \right].
              \]
            </p>

            <p>
              Предположим, что для заданной симметричной матрицы $W$ существует
              матрица Ляпунова $U(\tau)$. Очевидно, что матрица $U(\tau) + U_0(\tau)$
              также будет матрицей Ляпунова для матрицы $W$, что противоречит условию
              теоремы. Значит, система (2.1) удовлетворяет условию Ляпунова.
            </p>
          </div>
        </div>
      </div>

      <div class="corollary">
        Матрица Ляпунова $U(\tau)$ для матрицы $W$ удовлетворяет дифференциальному
        уравнению 2-го порядка:
        \[
        \ddv2{U(\tau)}{\tau}
        =
        \dv{U(\tau)}{\tau} A_0 - A_0^T \dv{U(\tau)}{\tau} + A_0^T U(\tau) A_0
        - A_1^T U(\tau) A_0, \qquad \tau \geqslant 0
        \]
        и следующим граничным условиям:
        \[
        \begin{gathered}
        \at{\dv{U(\tau)}{\tau}}{\tau = +0}
        =
        U(0) A_0 + U^T(h) A_1, \\
        \at{\dv{U(\tau)}{\tau}}{\tau = +0}
        +
        \paren{
        \at{\dv{U(\tau)}{\tau}}{\tau = +0}
        }^T
        = -W.
        \end{gathered}
        \]
      </div>

      <p>
        Рассмотрим теперь случай, когда система (2.1) не удовлетворяет условию Ляпунова.
      </p>

      <p>
        Докажем вспомогательный факт.
      </p>

      <div class="lemma">
        Пусть $\gamma, \mu \in \C^n$ &mdash; некоторые ненулевые векторы. Тогда существует
        вещественная симметричная матрица $W_0$ такая, что
        \[
        \gamma^T W_0 \mu \neq 0.
        \]

        <div class="proof">
          <div class="todo">
            расписать (стр. 55)
          </div>
        </div>
      </div>

      <div class="theorem">
        Если система (2.1) не удовлетворяет условию Ляпунова, то существует симметричная
        матрица $W$, для которой не существует матрицы Ляпунова.

        <div class="proof">
          <div class="todo">
            расписать (стр. 55)
          </div>
        </div>
      </div>

      <h3>Матрица Ляпунова: вычисление</h3>
      <div class="todo">Харитонов, стр. 56</div>

      <h3>Функционалы Ляпунова-Красовского полного типа</h3>
      <p>
        Одно из условий теоремы 2.3 утверждает, что найдётся такое $\alpha_1 \gt 0$,
        что для функционала $v(\varphi)$ выполняется условие
        \[
        \alpha_1 \norm{\varphi(0)}^2 \leqslant v(\varphi),
        \qquad
        \varphi \in PC([-h, 0], \R^n).
        \]
        Однако ни одной такой нижней границы не было найдено. Следующий пример
        подтверждает, что этой границы не существует.
      </p>

      <div class="example">
        <i>(Жабко А.П.)</i>
        <br />
        Скалярное уравнение
        \[
        \dv{x(t)}{t} = - x(t - 1), \qquad t \geqslant 0
        \]
        экспоненциально устойчиво. Это означает, что существует
        $\gamma \geqslant 1$ и $\sigma \gt 0$ такие, что для любого решения этого
        уравнения выполняется
        \[
        \abs{x(t, \varphi)} \leqslant \gamma e^{-\sigma t} \norm{\varphi}_1,
        \qquad t \geqslant 0.
        \]

        <p>
          Пусть $\varphi \in (0, 1)$. Определим начальную функцию
          \[
          \widetilde \varphi(\theta) =
          \begin{cases}
          \varepsilon, & \theta \in [-1, -1 + \varepsilon), \\
          0, & \theta \in [-1 + \varepsilon, 0), \\
          \varepsilon^2, & \theta = 0.
          \end{cases}
          \]
          Ясно, что
          \[
          \norm{\widetilde \varphi}_1
          = \sup_{\theta \in [-1, 0]} \abs{\widetilde \varphi(\theta)}
          = \varepsilon.
          \]
          Найдём соответствующее этой начальной функции решение $x(t, \widetilde \varphi)$
          методом шагов.
        </p>

        <p>
          Для $t \in [0,1]$:
          \[
          x(t, \widetilde \varphi) =
          \begin{cases}
          \varepsilon(\varepsilon - t), & t \in [0, \varepsilon], \\
          0, & t \in (\varepsilon, 1].
          \end{cases}
          \]
          Для $t \in [1, 2]$:
          \[
          x(t, \widetilde \varepsilon) =
          \begin{cases}
          -\varepsilon^2 (t - 1) + \frac{1}{2} \varepsilon(t - 1)^2,
          & t \in [1, 1 + \varepsilon] \\
          - \frac{1}{2} \varepsilon^3, & t \in (1 + \varepsilon, 2].
          \end{cases}
          \]
          Так как
          \[
          \abs{x(t, \widetilde \varphi)} \leqslant \frac{1}{2} \varepsilon^3,
          \qquad t \in [1,2],
          \]
          то экспоненциальная устойчивость влечёт за собой выполнение неравенства
          \[
          \abs{x(t, \widetilde \varphi)}
          \leqslant
          \gamma \frac{1}{2} \varepsilon^3 e^{-\sigma(t - 2)}
          \]
          при $t \geqslant 2$.
        </p>

        <p>
          Построим оценку для функционала $v_0(\widetilde \varphi)$:
          \[
          \begin{aligned}
          v_0(\widetilde \varphi)
          &=
          \int\limits_{0}^{\infty} x^2(t, \widetilde \varphi) dt = \\
          &=
          \int\limits_{0}^{1} x^2(t, \widetilde \varphi) dt
          +
          \int\limits_{1}^{2} x^2(t, \widetilde \varphi) dt
          +
          \int\limits_{2}^{\infty} x^2(t, \widetilde \varphi) dt \leqslant \\
          &\leqslant
          \frac{1}{3} \varepsilon^5 + \frac{1}{4} \paren{
          1 + \frac{\gamma^2}{2 \sigma}
          } \varepsilon^6.
          \end{aligned}
          \]
          Отсюда следует, что для функционала $v_0(\varphi)$ не существует
          нижней границы в виде
          \[
          \alpha_1 \abs{\varphi(0)}^2 \leqslant v_0(\varphi), \qquad \alpha_1 \gt 0,
          \]
          так как в противном случае неравенство
          \[
          \alpha_1 \varepsilon^2
          \leqslant
          \frac{1}{3} \varepsilon^5 + \frac{1}{4} \paren{
          1 + \frac{\gamma^2}{2 \sigma}
          } \varepsilon^6
          \]
          должно было бы выполняться при любом $\varepsilon \in (0, 1)$.
        </p>
      </div>

      <p>
        Из предыдущего примера следует, что требуется модифицировать функционал (2.13)
        \[
        \begin{aligned}
        v_0(\varphi)
        &=
        \varphi^T(0) U(0) \varphi(0)
        +
        2 \varphi^T(0) \int\limits_{-h}^{0} U(-h - \theta) A_1 \varphi(\theta) d\theta + \\
        &\phantom{=}
        + \int\limits_{-h}^{0} \varphi^T(\theta_1) A_1^T \left[
        \int\limits_{-h}^{0} U(\theta_1 - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] d \theta_1,
        \end{aligned}
        \]
        чтобы он удовлетворял условиям теоремы 2.3.
      </p>

      <div class="theorem">
        Рассмотрим три симметричные матрицы $W_0, W_1, W_2$. Пусть задан функционал
        \[
        \tag{2.32}
        w(\varphi)
        =
        \varphi^T(0) W_0 \varphi(0) + \varphi^T(-h) W_1 \varphi(-h)
        + \int\limits_{-h}^{0} \varphi^T(\theta) W_2 \varphi(\theta) d\theta.
        \]
        Если существует матрица Ляпунова $U(\tau)$ для матрицы $W = W_0 + W_1 + h W_2$,
        то функционал
        \[
        \tag{2.33}
        v(\varphi) = v_0(\varphi)
        + \int\limits_{-h}^{0} \varphi^T(\theta) \left[
        W_1 + (h + \theta) W_2
        \right] \varphi(\theta) d\theta
        \]
        вдоль решений системы (2.1) удовлетворяет уравнению
        \[
        \dv{}{t} v(x_t) = -w (x_t), \qquad t \geqslant 0,
        \]
        где $v_0(\varphi)$ &mdash; функционал (2.13)
        \[
        \begin{aligned}
        v_0(\varphi)
        &=
        \varphi^T(0) U(0) \varphi(0)
        +
        2 \varphi^T(0) \int\limits_{-h}^{0} U(-h - \theta) A_1 \varphi(\theta) d\theta + \\
        &\phantom{=}
        + \int\limits_{-h}^{0} \varphi^T(\theta_1) A_1^T \left[
        \int\limits_{-h}^{0} U(\theta_1 - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] d \theta_1
        \end{aligned}
        \]
        с матрицей Ляпунова $U(\tau)$.

        <div class="proof">
          Действительно, рассмотрим производную первого слагаемого $v_0(x_t)$:
          \[
          \dv{}{t} v_0(x_t) = - x^T(t) \left[ W_0 + W_1 + h W_2 \right] x(t).
          \]
          Производная второго слагаемого
          \[
          \begin{aligned}
          R(t)
          &=
          \int\limits_{-h}^{0} x^T(t + \theta) \left[
          W_1 + (h + \theta) W_2
          \right] x(t + \theta) d \theta = \Big< s = t + \theta \Big> = \\
          &=
          \int\limits_{t - h}^{t} x^T(s) \left[
          W_1 + (h + s - t) W_2
          \right] x(s) ds
          \end{aligned}
          \]
          равна
          \[
          \begin{aligned}
          \dv{}{t} R(t)
          &=
          x^T(t) \left[ W_1 + h W_2 \right] x(t) - x^T(t - h) W_1 x(t - h) - \\
          &\phantom{=}
          - \int\limits_{t - h}^{t} x^T(s) W_2 x(s) ds.
          \end{aligned}
          \]
          В сумме производные двух слагаемых дают $-w(x_t)$.
        </div>
      </div>

      <div class="definition">
        Говорят, что функционал (2.33)
        \[
        v(\varphi) = v_0(\varphi)
        + \int\limits_{-h}^{0} \varphi^T(\theta) \left[
        W_1 + (h + \theta) W_2
        \right] \varphi(\theta) d\theta,
        \]
        где
        \[
        \begin{aligned}
        v_0(\varphi)
        &=
        \varphi^T(0) U(0) \varphi(0)
        +
        2 \varphi^T(0) \int\limits_{-h}^{0} U(-h - \theta) A_1 \varphi(\theta) d\theta + \\
        &\phantom{=}
        + \int\limits_{-h}^{0} \varphi^T(\theta_1) A_1^T \left[
        \int\limits_{-h}^{0} U(\theta_1 - \theta_2) A_1 \varphi(\theta_2) d\theta_2
        \right] d \theta_1,
        \end{aligned}
        \]
        является <i>функционалом полного типа</i>, если матрицы $W_0, W_1, W_2$
        положительно определённые.
      </div>

      <div class="lemma">
        Пусть система (2.1) экспоненциально устойчива. Тогда для положительно-определённых
        матриц $W_0, W_1, W_2$ существует $\alpha_1 \gt 0$ такой, что функционал
        полного типа (2.33) допускает нижнюю границу в виде
        \[
        \alpha_1 \norm{\varphi(0)}^2 \leqslant v(\varphi),
        \qquad \varphi \in PC([-h, 0], \R^n).
        \]

        <div class="proof">
          <div class="todo">
            расписать (стр. 60)
          </div>
        </div>
      </div>

      <div class="lemma">
        Пусть система (2.1) удовлетворяет условию Ляпунова. Тогда для положительно
        определённых матриц $W_0, W_1, W_2$ существует $\alpha_2 \gt 0$ такой,
        что функционал полного типа (2.33) удовлетворяет неравенству
        \[
        v(\varphi) \leqslant \alpha_2 \norm{\varphi}_h^2,
        \qquad \varphi \in PC([-h, 0], \R^n).
        \]

        <div class="proof">
          <div class="todo">
            расписать (стр. 61)
          </div>
        </div>
      </div>

      <p>
        Возвращаясь к теореме 2.3, покажем, что её условия являются необходимыми
        для экспоненциальной устойчивости системы (2.1).
      </p>

      <div class="theorem">
        Система (2.1) является экспоненциально устойчивой тогда и только тогда,
        когда существует функционал
        \[
        v: PC([-h, 0], \R^n) \to \R,
        \]
        удовлетворяющий следующим условиям:
        <ol>
          <li>
            для некоторых положительных $\alpha_1, \alpha_2$ справедливо
            неравенство
            \[
            \alpha_1 \norm{\varphi(0)}^2
            \leqslant
            v(\varphi)
            \leqslant
            \alpha_2 \norm{\varphi}_h^2,
            \]
          </li>

          <li>
            для некоторого $\beta \gt 0$ неравенство
            \[
            \dv{}{t} v(x_t) \leqslant -\beta \norm{x(t)}^2, \qquad t \geqslant 0
            \]
            справедливо вдоль решений системы.
          </li>
        </ol>

        <div class="proof">
          <div class="necessity">
            Следует из лемм 2.10 и 2.11.
          </div>

          <div class="sufficiency">
            Следует из теоремы 2.3.
          </div>
        </div>
      </div>

      <h3>Приложения</h3>

      <h4>Квадратичный критерий качества</h4>
      <div class="todo">Харитонов, стр. 62</div>

      <h4>Экспоненциальное приближение</h4>

      <div class="lemma">
        Пусть система (2.1) экспоненциально устойчива. Тогда для положительно определённых
        матриц $W_0, W_1, W_2$ существуют положительные постоянные $\beta_1, \beta_2$
        такие, что функционал полного типа (2.33) удовлетворяет неравенству
        \[
        \tag{2.38}
        \beta_1 \norm{\varphi(0)}^2
        + \beta_2 \int\limits_{-h}^{0} \norm{\varphi(\theta)}^2 d\theta \leqslant v(\varphi),
        \qquad \varphi \in PC([-h, 0], \R^n).
        \]

        <div class="proof">
          <div class="todo">
            расписать (стр. 64)
          </div>
        </div>
      </div>

      <div class="lemma">
        Пусть система (2.1) удовлетворяет условию Ляпунова. Тогда для
        положительно определённых матриц $W_0, W_1, W_2$ существуют
        положительные постоянные $\delta_2, \delta_2$ такие, что функционал
        полного типа (2.33) удовлетворяет неравенству
        \[
        \tag{2.39}
        v(\varphi) \leqslant \delta_1 \norm{\varphi(0)}^2
        + \delta_2 \int\limits_{-h}^{0} \norm{\varphi(\theta)}^2 d\theta,
        \qquad \varphi \in PC([-h, 0], \R^n).
        \]

        <div class="proof">
          <div class="todo">
            расписать (стр. 65)
          </div>
        </div>
      </div>

      <p>
        Покажем, как экспоненциальная оценка решений системы (2.1) может быть получена
        с помощью функционалов полного типа.
      </p>

      <div class="theorem">
        Система (2.1) экспоненциально устойчива тогда и только тогда, когда
        для неё существует функционал полного типа $v(\varphi)$ такой, что
        для некоторых $\alpha_1, \alpha_2 \gt 0$ выполнено неравенство
        \[
        \alpha_1 \norm{\varphi(0)}^2
        \leqslant
        v(\varphi)
        \leqslant
        \alpha_2 \norm{\varphi}_h^2,
        \qquad \varphi \in PC([-h, 0], \R^n).
        \]

        <div class="proof">
          <div class="sufficiency">
            Пусть $v(\varphi)$ &mdash; функционал полного типа, удовлетворяющий
            условию теоремы. Тогда существуют положительно определённые матрицы
            $W_0, W_1, W_2$ такие, что
            \[
            \dv{}{t} v(x_t) = -w(x_t), \qquad t \geqslant 0,
            \]
            где
            \[
            w(x_t) = x^T(t) W_0 x(t) + x^T(t - h) W_1 x(t - h)
            + \int\limits_{-h}^{0} x^T(t + \theta) W_2 x(t + \theta) d\theta.
            \]

            <p>
              Покажем сначала, что существует $\sigma \gt 0$, для которой выполняется
              неравенство
              \[
              \tag{2.40}
              \dv{v(x_t)}{t} + 2 \sigma v(x_t) \leqslant 0,
              \qquad t \geqslant 0.
              \]
              Действительно, из леммы 2.13 следует, что существуют $\delta_1, \delta_2 \gt 0$
              такие, что
              \[
              v(\varphi) \leqslant \delta_1 \norm{\varphi(0)}^2
              + \delta_2 \int\limits_{-h}^{0} \norm{\varphi(\theta)}^2 d\theta,
              \qquad \varphi \in PC([-h, 0], \R^n).
              \]
              С другой стороны, очевидно, что
              \[
              w(\varphi)
              \geqslant
              \lambda_{\min}(W_0) \norm{\varphi(0)}^2
              +
              \lambda_{\min}(W_2) \int\limits_{-h}^{0} \norm{\varphi(\theta)}^2 d\theta,
              \qquad \varphi \in PC([-h, 0], \R^n),
              \]
              где $\lambda_{\min}(W)$ &mdash; минимальное собственное число матрицы $W$.
            </p>

            <p>
              Возьмём $\sigma \gt 0$ так, чтобы выполнялись неравенства
              \[
              2 \sigma \delta_1 \leqslant \lambda_{\min}(W_0),
              \qquad
              2 \sigma \delta_2 \leqslant \lambda_{\min}(W_2).
              \]
              Очевидно, что в этом случае неравенство (2.40) также будет выполнено.
            </p>

            <p>
              Из неравенства (2.40) следует, что
              \[
              v(x_t(\varphi)) \leqslant v(\varphi) e^{-2 \sigma t},
              \qquad t \geqslant 0.
              \]
              Тогда условие теоремы позволяет прийти к неравенствам
              \[
              \alpha_1 \norm{x(t, \varphi)}^2
              \leqslant
              v(x_t(\varphi))
              \leqslant
              v(\varphi) e^{-2 \sigma t}
              \leqslant
              \alpha_2 \norm{\varphi}_h^2 e^{-2 \sigma t},
              \qquad t \geqslant 0.
              \]
              Отсюда следует оценка
              \[
              \norm{x(t, \varphi)} \leqslant \gamma \norm{\varphi}_h e^{- \sigma t},
              \qquad t \geqslant 0,
              \]
              где
              \[
              \gamma = \sqrt{ \frac{\alpha_1}{\alpha_2} }.
              \]
            </p>
          </div>
        </div>
      </div>
    </div>
  </body>
</html>
